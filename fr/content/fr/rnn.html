
<!DOCTYPE html>


<html lang="fr" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Réseaux neuronaux récurrents &#8212; Introduction au Deep Learning (notes de cours)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=72dce1d2"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../../_static/translations.js?v=041d0952"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/fr/rnn';</script>
    <script src="../../_static/links.js?v=c5249c51"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Recherche" href="../../search.html" />
    <link rel="next" title="Mécanisme d’attention" href="attention.html" />
    <link rel="prev" title="Réseaux neuronaux convolutifs" href="convnets.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="fr"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Passer au contenu principal</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Haut de page</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction au Deep Learning (notes de cours)</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Recherche</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction au Deep Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="perceptron.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp.html">Perceptrons multicouches</a></li>
<li class="toctree-l1"><a class="reference internal" href="loss.html">Fonctions de coût</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">Optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="regularization.html">Régularisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="convnets.html">Réseaux neuronaux convolutifs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Réseaux neuronaux récurrents</a></li>
<li class="toctree-l1"><a class="reference internal" href="attention.html">Mécanisme d’attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="generative.html">Réseaux neuronaux génératifs</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/rtavenar/deep_book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Dépôt source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/rtavenar/deep_book/issues/new?title=Issue%20on%20page%20%2Fcontent/fr/rnn.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Ouvrez un problème"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Téléchargez cette page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/fr/rnn.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Télécharger le fichier source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Imprimer au format PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Mode plein écran"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="clair/sombre" aria-label="clair/sombre" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Recherche" aria-label="Recherche" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Réseaux neuronaux récurrents</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenu </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reseaux-recurrents-standard">Réseaux récurrents standard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#long-short-term-memory"><em>Long Short Term Memory</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gated-recurrent-unit">Gated Recurrent Unit</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">Références</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="reseaux-neuronaux-recurrents">
<span id="sec-rnn"></span><h1>Réseaux neuronaux récurrents<a class="headerlink" href="#reseaux-neuronaux-recurrents" title="Lien vers cette rubrique">#</a></h1>
<p>Les réseaux neuronaux récurrents (RNN) traitent les éléments d’une série temporelle un par un.
Typiquement, à l’instant <span class="math notranslate nohighlight">\(t\)</span>, un bloc récurrent prend en entrée :</p>
<ul class="simple">
<li><p>l’entrée courante <span class="math notranslate nohighlight">\(x_t\)</span> et</p></li>
<li><p>un état caché <span class="math notranslate nohighlight">\(h_{t-1}\)</span> qui a pour but de résumer les informations clés provenant de
des entrées passées <span class="math notranslate nohighlight">\(\{x_0, \dots, x_{t-1}\}\)</span></p></li>
</ul>
<p>Ce bloc retourne un état caché mis à jour <span class="math notranslate nohighlight">\(h_{t}\)</span> :</p>
<div class="figure" style="text-align: center"><p><img  src="../../_images/tikz-80f52ef9bafabd6b5809c28af7649ffd04631eb0.svg" alt="Figure made with TikZ" /></p>
</div><p>Il existe différentes couches récurrentes qui diffèrent principalement par la façon dont <span class="math notranslate nohighlight">\(h_t\)</span> est
calculée.</p>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;svg&#39;
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">notebook_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">prepare_notebook_graphics</span>
<span class="n">prepare_notebook_graphics</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="reseaux-recurrents-standard">
<h2>Réseaux récurrents standard<a class="headerlink" href="#reseaux-recurrents-standard" title="Lien vers cette rubrique">#</a></h2>
<p>La formulation originale d’une RNN est la suivante :</p>
<div class="amsmath math notranslate nohighlight" id="equation-9ed49132-cc31-433e-9d05-e5362d041088">
<span class="eqno">(5)<a class="headerlink" href="#equation-9ed49132-cc31-433e-9d05-e5362d041088" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \forall t, h_t = \text{tanh}(W_h h_{t-1} + W_x x_t + b)
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(W_h\)</span> est une matrice de poids associée au traitement de l’état caché précédent, <span class="math notranslate nohighlight">\(W_x\)</span> est une autre matrice de poids associée au traitement de la
l’entrée actuelle et <span class="math notranslate nohighlight">\(b\)</span> est un terme de biais.</p>
<p>On notera ici que <span class="math notranslate nohighlight">\(W_h\)</span>, <span class="math notranslate nohighlight">\(W_x\)</span> et <span class="math notranslate nohighlight">\(b\)</span> ne sont pas indexés par <span class="math notranslate nohighlight">\(t\)</span>, ce qui signifie que
qu’ils sont <strong>partagés entre tous les temps</strong>.</p>
<p>Une limitation importante de cette formule est qu’elle échoue à capturer les dépendances à long terme.
Pour mieux comprendre pourquoi, il faut se rappeler que les paramètres de ces réseaux sont optimisés par des  algorithmes de descente de gradient stochastique.</p>
<p>Pour simplifier les notations, considérons un cas simplifié dans lequel
<span class="math notranslate nohighlight">\(h_t\)</span> et <span class="math notranslate nohighlight">\(x_t\)</span> sont tous deux des valeurs scalaires, et regardons ce que vaut le gradient de la sortie <span class="math notranslate nohighlight">\(h_t\)</span> par rapport à <span class="math notranslate nohighlight">\(W_h\)</span> (qui est alors aussi un scalaire) :</p>
<div class="amsmath math notranslate nohighlight" id="equation-a78c0d2e-ce54-42b7-8602-6dbe23bcb326">
<span class="eqno">(6)<a class="headerlink" href="#equation-a78c0d2e-ce54-42b7-8602-6dbe23bcb326" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \nabla_{W_h}(h_t) = \text{tanh}^\prime(o_t) \cdot \frac{\partial o_t}{\partial W_h}
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(o_t = W_h h_{t-1} + W_x x_t + b\)</span>, donc:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e1b9311a-eeec-45fe-9c0e-286904a247e6">
<span class="eqno">(7)<a class="headerlink" href="#equation-e1b9311a-eeec-45fe-9c0e-286904a247e6" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \frac{\partial o_t}{\partial W_h} = h_{t-1} + W_h \cdot \frac{\partial h_{t-1}}{\partial W_h} \, .
\end{equation}\]</div>
<p>Ici, la forme de <span class="math notranslate nohighlight">\(\frac{\partial h_{t-1}}{\partial W_h}\)</span> sera similaire à
celle de <span class="math notranslate nohighlight">\(\nabla_{W_h}(h_t)\)</span> ci-dessus, et, au final, on obtient :</p>
<div class="amsmath math notranslate nohighlight" id="equation-0b865912-1864-4929-8f12-991c32dc756c">
<span class="eqno">(8)<a class="headerlink" href="#equation-0b865912-1864-4929-8f12-991c32dc756c" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
    \nabla_{W_h}(h_t) &amp;=&amp; \text{tanh}^\prime(o_t) \cdot
        \left[
            h_{t-1} + W_h \cdot \frac{\partial h_{t-1}}{\partial W_h}
        \right] \\
        &amp;=&amp; \text{tanh}^\prime(o_t) \cdot
           \left[
               h_{t-1} + W_h \cdot \text{tanh}^\prime(o_{t-1}) \cdot
               \left[
                   h_{t-2} + W_h \cdot \left[ \dots \right]
               \right]
           \right] \\
          &amp;=&amp; h_{t-1} \text{tanh}^\prime(o_t) + h_{t-2} W_h \text{tanh}^\prime(o_t) \text{tanh}^\prime(o_{t-1}) + \dots \\
         &amp;=&amp; \sum_{t^\prime = 1}^{t-1} h_{t^\prime} \left[ W_h^{t-t^\prime-1} \text{tanh}^\prime(o_{t^\prime+1}) \cdot \cdots \cdot  \text{tanh}^\prime(o_{t}) \right]
\end{eqnarray}\]</div>
<p>En d’autres termes, l’influence de <span class="math notranslate nohighlight">\(h_{t^\prime}\)</span> sera atténuée par un facteur
<span class="math notranslate nohighlight">\(W_h^{t-t^\prime-1} \text{tanh}^\prime(o_{t^\prime+1}) \cdot \cdots \cdot \text{tanh}^\prime(o_{t})\)</span>.</p>
<p>Rappelons maintenant à quoi ressemblent la fonction tanh et sa dérivée :</p>
<div class="cell tag_hide-input tag_remove-stderr docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">2.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="mf">1.</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tan_x</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">grad_tanh_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">tan_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">tan_x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tanh(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grad_tanh_x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tanh</span><span class="se">\&#39;</span><span class="s1">(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;on&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/9fa49cfa01a090344bbfcf64862db7097c337b943a02081914a25293dc9f2636.svg" src="../../_images/9fa49cfa01a090344bbfcf64862db7097c337b943a02081914a25293dc9f2636.svg" />
</div>
</div>
<p>On peut voir à quel point les gradients se rapprochent rapidement de 0 pour des entrées plus grandes (en valeur absolue) que 2, et avoir plusieurs termes de ce type dans une
dérivation en chaîne fera tendre les termes correspondants vers 0.</p>
<p>En d’autres termes, le gradient de l’état caché au temps <span class="math notranslate nohighlight">\(t\)</span> sera seulement
influencé par quelques uns de ses prédécesseurs <span class="math notranslate nohighlight">\(\{h_{t-1}, h_{t-2}, \dots\}\)</span> et les
les dépendances à long terme seront ignorées lors de l’actualisation des paramètres du modèle par
descente de gradient.
Il s’agit d’une occurrence d’un phénomène plus général connu sous le nom de <em>vanishing gradient</em>.</p>
</section>
<section id="long-short-term-memory">
<h2><em>Long Short Term Memory</em><a class="headerlink" href="#long-short-term-memory" title="Lien vers cette rubrique">#</a></h2>
<p>Les blocs <em>Long Short Term Memory</em> (LSTM, <span id="id1">[<a class="reference internal" href="#id23" title="Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.">Hochreiter and Schmidhuber, 1997</a>]</span>) ont été conçus comme une alternative à aux blocs récurrents classiques.
Ils visent à atténuer l’effet de <em>vanishing gradient</em> par l’utilisation de portes qui codent explicitement quelle partie de l’information doit (resp. ne doit pas) être utilisée.</p>
<div class="tip admonition">
<p class="admonition-title">Les portes dans les réseaux neuronaux</p>
<p>Dans la terminologie des réseaux de neurones, une porte <span class="math notranslate nohighlight">\(g \in [0, 1]^d\)</span> est un vecteur utilisé pour filtrer les informations d’un vecteur caractéristique entrant <span class="math notranslate nohighlight">\(v \in \mathbb{R}^d\)</span> de telle sorte que le résultat de l’application de la porte est : <span class="math notranslate nohighlight">\(g \odot v\)</span>.
où <span class="math notranslate nohighlight">\(\odot\)</span> est le produit élément-par-élément.
La porte <span class="math notranslate nohighlight">\(g\)</span> aura donc tendance à supprimer une partie des caractéristiques de <span class="math notranslate nohighlight">\(v\)</span>.
(celles qui correspondent à des valeurs très faibles de <span class="math notranslate nohighlight">\(g\)</span>).</p>
</div>
<p>Dans ces blocs, un état supplémentaire est utilisé, appelé état de la cellule <span class="math notranslate nohighlight">\(C_t\)</span>.
Cet état est calculé comme suit :</p>
<div class="amsmath math notranslate nohighlight" id="equation-c9653038-d8fa-46a4-80b5-a2c06d222800">
<span class="eqno">(9)<a class="headerlink" href="#equation-c9653038-d8fa-46a4-80b5-a2c06d222800" title="Permalink to this equation">#</a></span>\[\begin{equation}
    C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(f_t\)</span> est appelée <em>forget gate</em> (elle pousse le réseau à oublier les parties inutiles de l’état passé de la cellule),
<span class="math notranslate nohighlight">\(i_t\)</span> est l”<em>input gate</em> et <span class="math notranslate nohighlight">\(\tilde{C}_t\)</span> est une version actualisée de l’état de la cellule
(qui, à son tour, peut être partiellement censurée
par l”<em>input gate</em>).</p>
<p>Laissons de côté pour l’instant les détails concernant le calcul de ces 3 termes et concentrons-nous plutôt sur la façon dont la formule ci-dessus est est significativement différente de la règle de mise à jour de l’état caché dans le modèle classique.
En effet, dans ce cas, si le réseau l’apprend (par l’intermédiaire de <span class="math notranslate nohighlight">\(f_t\)</span>), l’information complète de l’état précédent  de la cellule <span class="math notranslate nohighlight">\(C_{t-1}\)</span> peut être récupérée,
ce qui permet aux gradients de se propager à rebours de l’axe du temps (et de ne plus disparaître).</p>
<p>Alors, le lien entre l’état de la cellule et l’état caché est :</p>
<div class="amsmath math notranslate nohighlight" id="equation-1c07e80f-ff12-4bfe-87e6-58b51a719185">
<span class="eqno">(10)<a class="headerlink" href="#equation-1c07e80f-ff12-4bfe-87e6-58b51a719185" title="Permalink to this equation">#</a></span>\[\begin{equation}
    h_t = o_t \odot \text{tanh}(C_{t}) \, .
\end{equation}\]</div>
<p>En d’autres termes, l’état caché est la version transformée (par la fonction tanh) de l’état de la cellule,
encore censuré par une porte de sortie (<em>output gate</em>) <span class="math notranslate nohighlight">\(o_t\)</span>.</p>
<p>Toutes les portes utilisées dans les formules ci-dessus sont définies de manière similaire :</p>
<div class="amsmath math notranslate nohighlight" id="equation-afaafee7-8cca-4f75-8fce-428402d0874a">
<span class="eqno">(11)<a class="headerlink" href="#equation-afaafee7-8cca-4f75-8fce-428402d0874a" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
    f_t &amp;=&amp; \sigma ( W_f \cdot [h_{t-1}, x_t] + b_f) \\
    i_t &amp;=&amp; \sigma ( W_i \cdot [h_{t-1}, x_t] + b_i) \\
    o_t &amp;=&amp; \sigma ( W_o \cdot [h_{t-1}, x_t] + b_o)
\end{eqnarray}\]</div>
<p>où <span class="math notranslate nohighlight">\(\sigma\)</span> est la fonction d’activation sigmoïde
(dont les valeurs sont comprises dans <span class="math notranslate nohighlight">\([0, 1]\)</span>) et
<span class="math notranslate nohighlight">\([h_{t-1}, x_t]\)</span> la concaténation des caractéristiques <span class="math notranslate nohighlight">\(h_{t-1}\)</span> et <span class="math notranslate nohighlight">\(x_t\)</span>.</p>
<p>Enfin, l’état de cellule mis à jour <span class="math notranslate nohighlight">\(\tilde{C}_t\)</span> est calculé comme suit :</p>
<div class="amsmath math notranslate nohighlight" id="equation-85f9db4b-daad-4ce5-96ff-1adb648f2562">
<span class="eqno">(12)<a class="headerlink" href="#equation-85f9db4b-daad-4ce5-96ff-1adb648f2562" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \tilde{C}_t = \text{tanh}(W_C \cdot [h_{t-1}, x_t] + b_C) \, .
\end{equation}\]</div>
<p>Il existe dans la littérature de nombreuses variantes de ces blocs LSTM qui reposent toujours sur les mêmes principes de base.</p>
</section>
<section id="gated-recurrent-unit">
<h2>Gated Recurrent Unit<a class="headerlink" href="#gated-recurrent-unit" title="Lien vers cette rubrique">#</a></h2>
<p>Une paramétrisation légèrement différente d’un bloc récurrent est utilisée dans les Gated Recurrent Units (GRU, <span id="id2">[<a class="reference internal" href="#id24" title="Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: encoder-decoder approaches. 2014. arXiv:1409.1259.">Cho <em>et al.</em>, 2014</a>]</span>).</p>
<p>Les GRUs reposent également sur l’utilisation de portes pour laisser (de manière adaptative) l’information circuler à travers le temps.
Une première différence significative entre les GRUs et les LSTMs est que les GRUs n’ont pas recours à l’utilisation d’un état de cellule.
Au lieu de cela, la règle de mise à jour de l’état caché est la suivante :</p>
<div class="amsmath math notranslate nohighlight" id="equation-1ee5c10b-bd1d-4593-b47b-d22610876d81">
<span class="eqno">(13)<a class="headerlink" href="#equation-1ee5c10b-bd1d-4593-b47b-d22610876d81" title="Permalink to this equation">#</a></span>\[\begin{equation}
    h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(z_t\)</span> est une porte qui équilibre (par caractéristique) la quantité d’informations
qui est conservée de l’état caché précédent avec la quantité d’informations
qui doit être mise à jour en utilisant le nouvel état caché candidat <span class="math notranslate nohighlight">\(\tilde{h}_t\)</span>,
calculé comme suit :</p>
<div class="amsmath math notranslate nohighlight" id="equation-014a01c9-2fa0-49f5-8b1e-cce0f4364fae">
<span class="eqno">(14)<a class="headerlink" href="#equation-014a01c9-2fa0-49f5-8b1e-cce0f4364fae" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \tilde{h}_t = \text{tanh}(W \cdot [r_t \odot h_{t-1}, x_t] + b) \, ,
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(r_t\)</span> est une porte supplémentaire qui peut cacher une partie de l’état caché précédent.</p>
<p>Les formules pour les portes <span class="math notranslate nohighlight">\(z_t\)</span> et <span class="math notranslate nohighlight">\(r_t\)</span> sont similaires à celles fournies pour <span class="math notranslate nohighlight">\(f_t\)</span>,
<span class="math notranslate nohighlight">\(i_t\)</span> et <span class="math notranslate nohighlight">\(o_t\)</span> dans le cas des LSTMs.</p>
<p>Une étude graphique de la capacité de ces variantes de réseaux récurrents à apprendre des dépendances à long terme est fournie
dans <span id="id3">[<a class="reference internal" href="#id28" title="Andreas Madsen. Visualizing memorization in rnns. Distill, 2019. URL: https://distill.pub/2019/memorization-in-rnns.">Madsen, 2019</a>]</span>.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Lien vers cette rubrique">#</a></h2>
<p>Dans ce chapitre et le précédent, nous avons passé en revue les architectures de réseaux de neurones qui sont utilisées pour apprendre à partir de données temporelles ou séquentielles.
En raison de contraintes de temps, nous n’avons pas abordé les modèles basés sur l’attention dans ce cours.
Nous avons présenté les modèles convolutifs qui visent à extraire des formes locales discriminantes dans les séries et les modèles récurrents qui exploitent plutôt la notion de séquence.
Concernant ces derniers, des variantes visant à faire face à l’effet de gradient évanescent ont été introduites.
Il est à noter que les modèles récurrents sont connus pour nécessiter plus de données d’entraînement que leurs homologues convolutifs.</p>
</section>
<section id="references">
<h2>Références<a class="headerlink" href="#references" title="Lien vers cette rubrique">#</a></h2>
<div class="docutils container" id="id4">
<div role="list" class="citation-list">
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">CVMerrienboerBB14</a><span class="fn-bracket">]</span></span>
<p>Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: encoder-decoder approaches. 2014. <a class="reference external" href="https://arxiv.org/abs/1409.1259">arXiv:1409.1259</a>.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">HS97</a><span class="fn-bracket">]</span></span>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. <em>Neural computation</em>, 9(8):1735–1780, 1997.</p>
</div>
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Mad19</a><span class="fn-bracket">]</span></span>
<p>Andreas Madsen. Visualizing memorization in rnns. <em>Distill</em>, 2019. URL: <a class="reference external" href="https://distill.pub/2019/memorization-in-rnns">https://distill.pub/2019/memorization-in-rnns</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/fr"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="convnets.html"
       title="page précédente">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">précédent</p>
        <p class="prev-next-title">Réseaux neuronaux convolutifs</p>
      </div>
    </a>
    <a class="right-next"
       href="attention.html"
       title="page suivante">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">suivant</p>
        <p class="prev-next-title">Mécanisme d’attention</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenu
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reseaux-recurrents-standard">Réseaux récurrents standard</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#long-short-term-memory"><em>Long Short Term Memory</em></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gated-recurrent-unit">Gated Recurrent Unit</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">Références</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Par Romain Tavenard
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div><a href="../../../book_fr.pdf">Télécharger ces notes en PDF</a><br />
<a href="../../../en/index.html" id="switch_lang">Switch to English</a></div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>