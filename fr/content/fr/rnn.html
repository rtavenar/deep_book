
<!DOCTYPE html>

<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Réseaux neuronaux récurrents &#8212; Introduction au Deep Learning (notes de cours)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Recherche" href="../../search.html" />
    <link rel="prev" title="Réseaux neuronaux convolutifs" href="convnets.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="fr">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Introduction au Deep Learning (notes de cours)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction au Deep Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mlp.html">
   Perceptrons multicouches
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss.html">
   Fonctions de coût
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optim.html">
   Optimisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regularization.html">
   Régularisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convnets.html">
   Réseaux neuronaux convolutifs
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Réseaux neuronaux récurrents
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <div><a href="../../../book_fr.pdf">Télécharger ces notes en PDF</a><br />
<a href="../../../en/index.html">Switch to English</a></div>

            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/rtavenar/deep_book/main?urlpath=tree/content/fr/rnn.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/rtavenar/deep_book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/rtavenar/deep_book/issues/new?title=Issue%20on%20page%20%2Fcontent/fr/rnn.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/content/fr/rnn.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../../_sources/content/fr/rnn.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reseaux-recurrents-standard">
   Réseaux récurrents standard
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#long-short-term-memory">
   <em>
    Long Short Term Memory
   </em>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-recurrent-unit">
   Gated Recurrent Unit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   Références
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Réseaux neuronaux récurrents</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reseaux-recurrents-standard">
   Réseaux récurrents standard
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#long-short-term-memory">
   <em>
    Long Short Term Memory
   </em>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-recurrent-unit">
   Gated Recurrent Unit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   Références
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="reseaux-neuronaux-recurrents">
<span id="sec-rnn"></span><h1>Réseaux neuronaux récurrents<a class="headerlink" href="#reseaux-neuronaux-recurrents" title="Lien permanent vers ce titre">#</a></h1>
<p>Les réseaux neuronaux récurrents (RNN) traitent les éléments d’une série temporelle un par un.
Typiquement, à l’instant <span class="math notranslate nohighlight">\(t\)</span>, un bloc récurrent prend en entrée :</p>
<ul class="simple">
<li><p>l’entrée courante <span class="math notranslate nohighlight">\(x_t\)</span> et</p></li>
<li><p>un état caché <span class="math notranslate nohighlight">\(h_{t-1}\)</span> qui a pour but de résumer les informations clés provenant de
des entrées passées <span class="math notranslate nohighlight">\(\{x_0, \dots, x_{t-1}\}\)</span></p></li>
</ul>
<p>Ce bloc retourne un état caché mis à jour <span class="math notranslate nohighlight">\(h_{t}\)</span> :</p>
<div class="figure" style="text-align: center"><p><img  src="../../_images/tikz-80f52ef9bafabd6b5809c28af7649ffd04631eb0.svg" alt="Figure made with TikZ" /></p>
</div><p>Il existe différentes couches récurrentes qui diffèrent principalement par la façon dont <span class="math notranslate nohighlight">\(h_t\)</span> est
calculée.</p>
<div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;svg&#39;
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">notebook_utils</span> <span class="kn">import</span> <span class="n">prepare_notebook_graphics</span>
<span class="n">prepare_notebook_graphics</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="reseaux-recurrents-standard">
<h2>Réseaux récurrents standard<a class="headerlink" href="#reseaux-recurrents-standard" title="Lien permanent vers ce titre">#</a></h2>
<p>La formulation originale d’une RNN est la suivante :</p>
<div class="amsmath math notranslate nohighlight" id="equation-922a119d-6356-407e-be3d-c5a80812c0e9">
<span class="eqno">(5)<a class="headerlink" href="#equation-922a119d-6356-407e-be3d-c5a80812c0e9" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    \forall t, h_t = \text{tanh}(W_h h_{t-1} + W_x x_t + b)
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(W_h\)</span> est une matrice de poids associée au traitement de l’état caché précédent, <span class="math notranslate nohighlight">\(W_x\)</span> est une autre matrice de poids associée au traitement de la
l’entrée actuelle et <span class="math notranslate nohighlight">\(b\)</span> est un terme de biais.</p>
<p>On notera ici que <span class="math notranslate nohighlight">\(W_h\)</span>, <span class="math notranslate nohighlight">\(W_x\)</span> et <span class="math notranslate nohighlight">\(b\)</span> ne sont pas indexés par <span class="math notranslate nohighlight">\(t\)</span>, ce qui signifie que
qu’ils sont <strong>partagés entre tous les temps</strong>.</p>
<p>Une limitation importante de cette formule est qu’elle échoue à capturer les dépendances à long terme.
Pour mieux comprendre pourquoi, il faut se rappeler que les paramètres de ces réseaux sont optimisés par des  algorithmes de descente de gradient stochastique.</p>
<p>Pour simplifier les notations, considérons un cas simplifié dans lequel
<span class="math notranslate nohighlight">\(h_t\)</span> et <span class="math notranslate nohighlight">\(x_t\)</span> sont tous deux des valeurs scalaires, et regardons ce que vaut le gradient de la sortie <span class="math notranslate nohighlight">\(h_t\)</span> par rapport à <span class="math notranslate nohighlight">\(W_h\)</span> (qui est alors aussi un scalaire) :</p>
<div class="amsmath math notranslate nohighlight" id="equation-25ff0e60-bd5d-4e3e-874a-c9dbf63a3ee3">
<span class="eqno">(6)<a class="headerlink" href="#equation-25ff0e60-bd5d-4e3e-874a-c9dbf63a3ee3" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    \nabla_{W_h}(h_t) = \text{tanh}^\prime(o_t) \cdot \frac{\partial o_t}{\partial W_h}
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(o_t = W_h h_{t-1} + W_x x_t + b\)</span>, donc:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e21f7b8c-0801-4e40-a5a3-91986d72f36e">
<span class="eqno">(7)<a class="headerlink" href="#equation-e21f7b8c-0801-4e40-a5a3-91986d72f36e" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    \frac{\partial o_t}{\partial W_h} = h_{t-1} + W_h \cdot \frac{\partial h_{t-1}}{\partial W_h} \, .
\end{equation}\]</div>
<p>Ici, la forme de <span class="math notranslate nohighlight">\(\frac{\partial h_{t-1}}{\partial W_h}\)</span> sera similaire à
celle de <span class="math notranslate nohighlight">\(\nabla_{W_h}(h_t)\)</span> ci-dessus, et, au final, on obtient :</p>
<div class="amsmath math notranslate nohighlight" id="equation-b3df9028-7b96-484a-abea-746093e5bf25">
<span class="eqno">(8)<a class="headerlink" href="#equation-b3df9028-7b96-484a-abea-746093e5bf25" title="Lien permanent vers cette équation">#</a></span>\[\begin{eqnarray}
    \nabla_{W_h}(h_t) &amp;=&amp; \text{tanh}^\prime(o_t) \cdot
        \left[
            h_{t-1} + W_h \cdot \frac{\partial h_{t-1}}{\partial W_h}
        \right] \\
        &amp;=&amp; \text{tanh}^\prime(o_t) \cdot
           \left[
               h_{t-1} + W_h \cdot \text{tanh}^\prime(o_{t-1}) \cdot
               \left[
                   h_{t-2} + W_h \cdot \left[ \dots \right]
               \right]
           \right] \\
          &amp;=&amp; h_{t-1} \text{tanh}^\prime(o_t) + h_{t-2} W_h \text{tanh}^\prime(o_t) \text{tanh}^\prime(o_{t-1}) + \dots \\
         &amp;=&amp; \sum_{t^\prime = 1}^{t-1} h_{t^\prime} \left[ W_h^{t-t^\prime-1} \text{tanh}^\prime(o_{t^\prime+1}) \cdot \cdots \cdot  \text{tanh}^\prime(o_{t}) \right]
\end{eqnarray}\]</div>
<p>En d’autres termes, l’influence de <span class="math notranslate nohighlight">\(h_{t^\prime}\)</span> sera atténuée par un facteur
<span class="math notranslate nohighlight">\(W_h^{t-t^\prime-1} \text{tanh}^\prime(o_{t^\prime+1}) \cdot \cdots \cdot \text{tanh}^\prime(o_{t})\)</span>.</p>
<p>Rappelons maintenant à quoi ressemblent la fonction tanh et sa dérivée :</p>
<div class="cell tag_hide-input tag_remove-stderr docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">2.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="mf">1.</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">tan_x</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">grad_tanh_x</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">tan_x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">tan_x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tanh(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">grad_tanh_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tanh</span><span class="se">\&#39;</span><span class="s1">(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;on&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/rnn_3_2.svg" src="../../_images/rnn_3_2.svg" /></div>
</div>
<p>On peut voir à quel point les gradients se rapprochent rapidement de 0 pour des entrées plus grandes (en valeur absolue) que 2, et avoir plusieurs termes de ce type dans une
dérivation en chaîne fera tendre les termes correspondants vers 0.</p>
<p>En d’autres termes, le gradient de l’état caché au temps <span class="math notranslate nohighlight">\(t\)</span> sera seulement
influencé par quelques uns de ses prédécesseurs <span class="math notranslate nohighlight">\(\{h_{t-1}, h_{t-2}, \dots\}\)</span> et les
les dépendances à long terme seront ignorées lors de l’actualisation des paramètres du modèle par
descente de gradient.
Il s’agit d’une occurrence d’un phénomène plus général connu sous le nom de <em>vanishing gradient</em>.</p>
</section>
<section id="long-short-term-memory">
<h2><em>Long Short Term Memory</em><a class="headerlink" href="#long-short-term-memory" title="Lien permanent vers ce titre">#</a></h2>
<p>Les blocs <em>Long Short Term Memory</em> (LSTM, <span id="id1">[<a class="reference internal" href="#id23" title="Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.">Hochreiter and Schmidhuber, 1997</a>]</span>) ont été conçus comme une alternative à aux blocs récurrents classiques.
Ils visent à atténuer l’effet de <em>vanishing gradient</em> par l’utilisation de portes qui codent explicitement quelle partie de l’information doit (resp. ne doit pas) être utilisée.</p>
<div class="tip admonition">
<p class="admonition-title">Les portes dans les réseaux neuronaux</p>
<p>Dans la terminologie des réseaux de neurones, une porte <span class="math notranslate nohighlight">\(g \in [0, 1]^d\)</span> est un vecteur utilisé pour filtrer les informations d’un vecteur caractéristique entrant <span class="math notranslate nohighlight">\(v \in \mathbb{R}^d\)</span> de telle sorte que le résultat de l’application de la porte est : <span class="math notranslate nohighlight">\(g \odot v\)</span>.
où <span class="math notranslate nohighlight">\(\odot\)</span> est le produit élément-par-élément.
La porte <span class="math notranslate nohighlight">\(g\)</span> aura donc tendance à supprimer une partie des caractéristiques de <span class="math notranslate nohighlight">\(v\)</span>.
(celles qui correspondent à des valeurs très faibles de <span class="math notranslate nohighlight">\(g\)</span>).</p>
</div>
<p>Dans ces blocs, un état supplémentaire est utilisé, appelé état de la cellule <span class="math notranslate nohighlight">\(C_t\)</span>.
Cet état est calculé comme suit :</p>
<div class="amsmath math notranslate nohighlight" id="equation-ed725785-0153-4c2f-ac82-2fa66e60aa79">
<span class="eqno">(9)<a class="headerlink" href="#equation-ed725785-0153-4c2f-ac82-2fa66e60aa79" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(f_t\)</span> est appelée <em>forget gate</em> (elle pousse le réseau à oublier les parties inutiles de l’état passé de la cellule),
<span class="math notranslate nohighlight">\(i_t\)</span> est l”<em>input gate</em> et <span class="math notranslate nohighlight">\(\tilde{C}_t\)</span> est une version actualisée de l’état de la cellule
(qui, à son tour, peut être partiellement censurée
par l”<em>input gate</em>).</p>
<p>Laissons de côté pour l’instant les détails concernant le calcul de ces 3 termes et concentrons-nous plutôt sur la façon dont la formule ci-dessus est est significativement différente de la règle de mise à jour de l’état caché dans le modèle classique.
En effet, dans ce cas, si le réseau l’apprend (par l’intermédiaire de <span class="math notranslate nohighlight">\(f_t\)</span>), l’information complète de l’état précédent  de la cellule <span class="math notranslate nohighlight">\(C_{t-1}\)</span> peut être récupérée,
ce qui permet aux gradients de se propager à rebours de l’axe du temps (et de ne plus disparaître).</p>
<p>Alors, le lien entre l’état de la cellule et l’état caché est :</p>
<div class="amsmath math notranslate nohighlight" id="equation-35728f7e-c4d5-4bfd-b1c3-b6661352cbbb">
<span class="eqno">(10)<a class="headerlink" href="#equation-35728f7e-c4d5-4bfd-b1c3-b6661352cbbb" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    h_t = o_t \odot \text{tanh}(C_{t}) \, .
\end{equation}\]</div>
<p>En d’autres termes, l’état caché est la version transformée (par la fonction tanh) de l’état de la cellule,
encore censuré par une porte de sortie (<em>output gate</em>) <span class="math notranslate nohighlight">\(o_t\)</span>.</p>
<p>Toutes les portes utilisées dans les formules ci-dessus sont définies de manière similaire :</p>
<div class="amsmath math notranslate nohighlight" id="equation-b9e8c5fb-8ce4-4ca4-934b-3556ca0d268c">
<span class="eqno">(11)<a class="headerlink" href="#equation-b9e8c5fb-8ce4-4ca4-934b-3556ca0d268c" title="Lien permanent vers cette équation">#</a></span>\[\begin{eqnarray}
    f_t &amp;=&amp; \sigma ( W_f \cdot [h_{t-1}, x_t] + b_f) \\
    i_t &amp;=&amp; \sigma ( W_i \cdot [h_{t-1}, x_t] + b_i) \\
    o_t &amp;=&amp; \sigma ( W_o \cdot [h_{t-1}, x_t] + b_o)
\end{eqnarray}\]</div>
<p>où <span class="math notranslate nohighlight">\(\sigma\)</span> est la fonction d’activation sigmoïde
(dont les valeurs sont comprises dans <span class="math notranslate nohighlight">\([0, 1]\)</span>) et
<span class="math notranslate nohighlight">\([h_{t-1}, x_t]\)</span> la concaténation des caractéristiques <span class="math notranslate nohighlight">\(h_{t-1}\)</span> et <span class="math notranslate nohighlight">\(x_t\)</span>.</p>
<p>Enfin, l’état de cellule mis à jour <span class="math notranslate nohighlight">\(\tilde{C}_t\)</span> est calculé comme suit :</p>
<div class="amsmath math notranslate nohighlight" id="equation-3b95737b-9f42-42e6-9e95-a4f056544bd5">
<span class="eqno">(12)<a class="headerlink" href="#equation-3b95737b-9f42-42e6-9e95-a4f056544bd5" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    \tilde{C}_t = \text{tanh}(W_C \cdot [h_{t-1}, x_t] + b_C) \, .
\end{equation}\]</div>
<p>Il existe dans la littérature de nombreuses variantes de ces blocs LSTM qui reposent toujours sur les mêmes principes de base.</p>
</section>
<section id="gated-recurrent-unit">
<h2>Gated Recurrent Unit<a class="headerlink" href="#gated-recurrent-unit" title="Lien permanent vers ce titre">#</a></h2>
<p>Une paramétrisation légèrement différente d’un bloc récurrent est utilisée dans les Gated Recurrent Units (GRU, <span id="id2">[<a class="reference internal" href="#id24" title="Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: encoder-decoder approaches. 2014. arXiv:1409.1259.">Cho <em>et al.</em>, 2014</a>]</span>).</p>
<p>Les GRUs reposent également sur l’utilisation de portes pour laisser (de manière adaptative) l’information circuler à travers le temps.
Une première différence significative entre les GRUs et les LSTMs est que les GRUs n’ont pas recours à l’utilisation d’un état de cellule.
Au lieu de cela, la règle de mise à jour de l’état caché est la suivante :</p>
<div class="amsmath math notranslate nohighlight" id="equation-c3acabc6-9632-44fb-a8dd-30a59ec20f62">
<span class="eqno">(13)<a class="headerlink" href="#equation-c3acabc6-9632-44fb-a8dd-30a59ec20f62" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(z_t\)</span> est une porte qui équilibre (par caractéristique) la quantité d’informations
qui est conservée de l’état caché précédent avec la quantité d’informations
qui doit être mise à jour en utilisant le nouvel état caché candidat <span class="math notranslate nohighlight">\(\tilde{h}_t\)</span>,
calculé comme suit :</p>
<div class="amsmath math notranslate nohighlight" id="equation-668eb29f-3fbd-4dde-b951-941de4be1352">
<span class="eqno">(14)<a class="headerlink" href="#equation-668eb29f-3fbd-4dde-b951-941de4be1352" title="Lien permanent vers cette équation">#</a></span>\[\begin{equation}
    \tilde{h}_t = \text{tanh}(W \cdot [r_t \odot h_{t-1}, x_t] + b) \, ,
\end{equation}\]</div>
<p>où <span class="math notranslate nohighlight">\(r_t\)</span> est une porte supplémentaire qui peut cacher une partie de l’état caché précédent.</p>
<p>Les formules pour les portes <span class="math notranslate nohighlight">\(z_t\)</span> et <span class="math notranslate nohighlight">\(r_t\)</span> sont similaires à celles fournies pour <span class="math notranslate nohighlight">\(f_t\)</span>,
<span class="math notranslate nohighlight">\(i_t\)</span> et <span class="math notranslate nohighlight">\(o_t\)</span> dans le cas des LSTMs.</p>
<p>Une étude graphique de la capacité de ces variantes de réseaux récurrents à apprendre des dépendances à long terme est fournie
dans <span id="id3">[<a class="reference internal" href="#id28" title="Andreas Madsen. Visualizing memorization in rnns. Distill, 2019. URL: https://distill.pub/2019/memorization-in-rnns.">Madsen, 2019</a>]</span>.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Lien permanent vers ce titre">#</a></h2>
<p>Dans ce chapitre et le précédent, nous avons passé en revue les architectures de réseaux de neurones qui sont utilisées pour apprendre à partir de données temporelles ou séquentielles.
En raison de contraintes de temps, nous n’avons pas abordé les modèles basés sur l’attention dans ce cours.
Nous avons présenté les modèles convolutifs qui visent à extraire des formes locales discriminantes dans les séries et les modèles récurrents qui exploitent plutôt la notion de séquence.
Concernant ces derniers, des variantes visant à faire face à l’effet de gradient évanescent ont été introduites.
Il est à noter que les modèles récurrents sont connus pour nécessiter plus de données d’entraînement que leurs homologues convolutifs.</p>
</section>
<section id="references">
<h2>Références<a class="headerlink" href="#references" title="Lien permanent vers ce titre">#</a></h2>
<div class="docutils container" id="id4">
<dl class="citation">
<dt class="label" id="id24"><span class="brackets"><a class="fn-backref" href="#id2">CVMerrienboerBB14</a></span></dt>
<dd><p>Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: encoder-decoder approaches. 2014. <a class="reference external" href="https://arxiv.org/abs/1409.1259">arXiv:1409.1259</a>.</p>
</dd>
<dt class="label" id="id23"><span class="brackets"><a class="fn-backref" href="#id1">HS97</a></span></dt>
<dd><p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. <em>Neural computation</em>, 9(8):1735–1780, 1997.</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id3">Mad19</a></span></dt>
<dd><p>Andreas Madsen. Visualizing memorization in rnns. <em>Distill</em>, 2019. URL: <a class="reference external" href="https://distill.pub/2019/memorization-in-rnns">https://distill.pub/2019/memorization-in-rnns</a>.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/fr"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="convnets.html" title="précédent page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">précédent</p>
            <p class="prev-next-title">Réseaux neuronaux convolutifs</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Romain Tavenard<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>