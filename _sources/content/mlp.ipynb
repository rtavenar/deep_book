{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b457e21",
   "metadata": {},
   "source": [
    "(sec:mlp)=\n",
    "# Multi Layer Perceptrons (MLP)\n",
    "\n",
    "In the previous chapter, we have seen a very simple model called the Perceptron.\n",
    "In this model, the predicted output $\\hat{y}$ is computed as a linear combination of the input features plus a bias:\n",
    "\n",
    "$$\\hat{y} = \\sum_{j=1}^d x_j w_j + b$$\n",
    "\n",
    "In other words, we were optimizing among the family of linear models, which is a quite restricted family.\n",
    "In order to cover a wider range of models, one can stack neurons organized in layers to form a more complex model, such as the model below, which is called a one-hidden-layer model, since an extra layer of neurons is introduced between the inputs and the output:\n",
    "\n",
    "```{tikz}\n",
    "    \\node[text width=3cm, align=center] (in_title) at  (0, 6) {Input layer\\\\ $\\mathbf{x}$};\n",
    "    \\node[text width=3cm, align=center] (h1_title) at  (3, 6) {Hidden layer 1\\\\ $\\mathbf{h^{(1)}}$};\n",
    "    \\node[text width=3cm, align=center] (out_title) at  (6, 6) {Output layer\\\\ $\\mathbf{\\hat{y}}$};\n",
    "\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in0) at  (0, 4) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in1) at  (0, 3) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in2) at  (0, 2) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in3) at  (0, 1) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in4) at  (0, 0) {};\n",
    "\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_0) at  (3, 5) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_1) at  (3, 4) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_2) at  (3, 3) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_3) at  (3, 2) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_4) at  (3, 1) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_5) at  (3, 0) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_6) at  (3, -1) {};\n",
    "    \n",
    "    \\node[draw, circle, fill=green, minimum size=17pt,inner sep=0pt] (out_0) at  (6, 2) {};\n",
    "    \\draw[->] (in0) -- (h1_0);\n",
    "    \\draw[->] (in0) -- (h1_1);\n",
    "    \\draw[->] (in0) -- (h1_2);\n",
    "    \\draw[->] (in0) -- (h1_3);\n",
    "    \\draw[->] (in0) -- (h1_4);\n",
    "    \\draw[->] (in0) -- (h1_5);\n",
    "    \\draw[->] (in0) -- (h1_6);\n",
    "    \\draw[->] (in1) -- (h1_0);\n",
    "    \\draw[->] (in1) -- (h1_1);\n",
    "    \\draw[->] (in1) -- (h1_2);\n",
    "    \\draw[->] (in1) -- (h1_3);\n",
    "    \\draw[->] (in1) -- (h1_4);\n",
    "    \\draw[->] (in1) -- (h1_5);\n",
    "    \\draw[->] (in1) -- (h1_6);\n",
    "    \\draw[->] (in2) -- (h1_0);\n",
    "    \\draw[->] (in2) -- (h1_1);\n",
    "    \\draw[->] (in2) -- (h1_2);\n",
    "    \\draw[->] (in2) -- (h1_3);\n",
    "    \\draw[->] (in2) -- (h1_4);\n",
    "    \\draw[->] (in2) -- (h1_5);\n",
    "    \\draw[->] (in2) -- (h1_6);\n",
    "    \\draw[->] (in3) -- (h1_0);\n",
    "    \\draw[->] (in3) -- (h1_1);\n",
    "    \\draw[->] (in3) -- (h1_2);\n",
    "    \\draw[->] (in3) -- (h1_3);\n",
    "    \\draw[->] (in3) -- (h1_4);\n",
    "    \\draw[->] (in3) -- (h1_5);\n",
    "    \\draw[->] (in3) -- (h1_6);\n",
    "    \\draw[->] (in4) -- (h1_0);\n",
    "    \\draw[->] (in4) -- (h1_1);\n",
    "    \\draw[->] (in4) -- (h1_2);\n",
    "    \\draw[->] (in4) -- (h1_3);\n",
    "    \\draw[->] (in4) -- (h1_4);\n",
    "    \\draw[->] (in4) -- (h1_5);\n",
    "    \\draw[->] (in4) -- (h1_6);\n",
    "    \\draw[->] (h1_0) -- (out_0);\n",
    "    \\draw[->] (h1_1) -- (out_0);\n",
    "    \\draw[->] (h1_2) -- (out_0);\n",
    "    \\draw[->] (h1_3) -- (out_0);\n",
    "    \\draw[->] (h1_4) -- (out_0);\n",
    "    \\draw[->] (h1_5) -- (out_0);\n",
    "    \\draw[->] (h1_6) -- (out_0);\n",
    "\n",
    "\n",
    "    \\node[fill=white] (beta0) at  (1.5, 2) {$\\mathbf{w^{(0)}}$};\n",
    "    \\node[fill=white] (beta1) at  (4.5, 2) {$\\mathbf{w^{(1)}}$};\n",
    "```\n",
    "\n",
    "The question one might ask now is whether this added hidden layer effectively allows to cover a wider family of models.\n",
    "This is what the Universal Approximation Theorem below is all about.\n",
    "\n",
    "```{admonition} Universal Approximation Theorem\n",
    "\n",
    "The Universal Approximation Theorem states that any continuous function defined on a compact set can be \n",
    "approximated as closely as one wants by a one-hidden-layer neural network with sigmoid activation.\n",
    "```\n",
    "\n",
    "In other words, by using a hidden layer to map inputs to outputs, one can now approximate any continuous function, which is a very interesting property.\n",
    "Note however that the number of hidden neurons that is necessary to achieve a given approximation quality is not discussed here.\n",
    "Moreover, it is not sufficient that such a good approximation exists, another important question is whether the optimization algorithms we will use will eventually converge to this solution or not, which is not guaranteed, as discussed in more details in [the dedicqted chapter](sec:sgd).\n",
    "\n",
    "In practice, we observe empirically that in order to achieve a given approximation quality, it is more efficient (in terms of the number of parameters required) to stack several hidden layers rather than rely on a single one :\n",
    "\n",
    "```{tikz}\n",
    "    \\node[text width=3cm, align=center] (in_title) at  (0, 6) {Input layer\\\\ $\\mathbf{x}$};\n",
    "    \\node[text width=3cm, align=center] (h1_title) at  (3, 6) {Hidden layer 1\\\\ $\\mathbf{h^{(1)}}$};\n",
    "    \\node[text width=3cm, align=center] (h1_title) at  (6, 6) {Hidden layer 2\\\\ $\\mathbf{h^{(2)}}$};\n",
    "    \\node[text width=3cm, align=center] (out_title) at  (9, 6) {Output layer\\\\ $\\mathbf{\\hat{y}}$};\n",
    "\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in0) at  (0, 4) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in1) at  (0, 3) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in2) at  (0, 2) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in3) at  (0, 1) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in4) at  (0, 0) {};\n",
    "\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_0) at  (3, 5) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_1) at  (3, 4) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_2) at  (3, 3) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_3) at  (3, 2) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_4) at  (3, 1) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_5) at  (3, 0) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_6) at  (3, -1) {};\n",
    "\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h2_0) at  (6, 5) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h2_1) at  (6, 4) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h2_2) at  (6, 3) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h2_3) at  (6, 2) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h2_4) at  (6, 1) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h2_5) at  (6, 0) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h2_6) at  (6, -1) {};\n",
    "    \n",
    "    \\node[draw, circle, fill=green, minimum size=17pt,inner sep=0pt] (out_0) at  (9, 2) {};\n",
    "    \\draw[->] (in0) -- (h1_0);\n",
    "    \\draw[->] (in0) -- (h1_1);\n",
    "    \\draw[->] (in0) -- (h1_2);\n",
    "    \\draw[->] (in0) -- (h1_3);\n",
    "    \\draw[->] (in0) -- (h1_4);\n",
    "    \\draw[->] (in0) -- (h1_5);\n",
    "    \\draw[->] (in0) -- (h1_6);\n",
    "    \\draw[->] (in1) -- (h1_0);\n",
    "    \\draw[->] (in1) -- (h1_1);\n",
    "    \\draw[->] (in1) -- (h1_2);\n",
    "    \\draw[->] (in1) -- (h1_3);\n",
    "    \\draw[->] (in1) -- (h1_4);\n",
    "    \\draw[->] (in1) -- (h1_5);\n",
    "    \\draw[->] (in1) -- (h1_6);\n",
    "    \\draw[->] (in2) -- (h1_0);\n",
    "    \\draw[->] (in2) -- (h1_1);\n",
    "    \\draw[->] (in2) -- (h1_2);\n",
    "    \\draw[->] (in2) -- (h1_3);\n",
    "    \\draw[->] (in2) -- (h1_4);\n",
    "    \\draw[->] (in2) -- (h1_5);\n",
    "    \\draw[->] (in2) -- (h1_6);\n",
    "    \\draw[->] (in3) -- (h1_0);\n",
    "    \\draw[->] (in3) -- (h1_1);\n",
    "    \\draw[->] (in3) -- (h1_2);\n",
    "    \\draw[->] (in3) -- (h1_3);\n",
    "    \\draw[->] (in3) -- (h1_4);\n",
    "    \\draw[->] (in3) -- (h1_5);\n",
    "    \\draw[->] (in3) -- (h1_6);\n",
    "    \\draw[->] (in4) -- (h1_0);\n",
    "    \\draw[->] (in4) -- (h1_1);\n",
    "    \\draw[->] (in4) -- (h1_2);\n",
    "    \\draw[->] (in4) -- (h1_3);\n",
    "    \\draw[->] (in4) -- (h1_4);\n",
    "    \\draw[->] (in4) -- (h1_5);\n",
    "    \\draw[->] (in4) -- (h1_6);\n",
    "\n",
    "    \\draw[->] (h1_0) -- (h2_0);\n",
    "    \\draw[->] (h1_1) -- (h2_0);\n",
    "    \\draw[->] (h1_2) -- (h2_0);\n",
    "    \\draw[->] (h1_3) -- (h2_0);\n",
    "    \\draw[->] (h1_4) -- (h2_0);\n",
    "    \\draw[->] (h1_5) -- (h2_0);\n",
    "    \\draw[->] (h1_6) -- (h2_0);\n",
    "    \\draw[->] (h1_0) -- (h2_1);\n",
    "    \\draw[->] (h1_1) -- (h2_1);\n",
    "    \\draw[->] (h1_2) -- (h2_1);\n",
    "    \\draw[->] (h1_3) -- (h2_1);\n",
    "    \\draw[->] (h1_4) -- (h2_1);\n",
    "    \\draw[->] (h1_5) -- (h2_1);\n",
    "    \\draw[->] (h1_6) -- (h2_1);\n",
    "    \\draw[->] (h1_0) -- (h2_2);\n",
    "    \\draw[->] (h1_1) -- (h2_2);\n",
    "    \\draw[->] (h1_2) -- (h2_2);\n",
    "    \\draw[->] (h1_3) -- (h2_2);\n",
    "    \\draw[->] (h1_4) -- (h2_2);\n",
    "    \\draw[->] (h1_5) -- (h2_2);\n",
    "    \\draw[->] (h1_6) -- (h2_2);\n",
    "    \\draw[->] (h1_0) -- (h2_3);\n",
    "    \\draw[->] (h1_1) -- (h2_3);\n",
    "    \\draw[->] (h1_2) -- (h2_3);\n",
    "    \\draw[->] (h1_3) -- (h2_3);\n",
    "    \\draw[->] (h1_4) -- (h2_3);\n",
    "    \\draw[->] (h1_5) -- (h2_3);\n",
    "    \\draw[->] (h1_6) -- (h2_3);\n",
    "    \\draw[->] (h1_0) -- (h2_4);\n",
    "    \\draw[->] (h1_1) -- (h2_4);\n",
    "    \\draw[->] (h1_2) -- (h2_4);\n",
    "    \\draw[->] (h1_3) -- (h2_4);\n",
    "    \\draw[->] (h1_4) -- (h2_4);\n",
    "    \\draw[->] (h1_5) -- (h2_4);\n",
    "    \\draw[->] (h1_6) -- (h2_4);\n",
    "    \\draw[->] (h1_0) -- (h2_5);\n",
    "    \\draw[->] (h1_1) -- (h2_5);\n",
    "    \\draw[->] (h1_2) -- (h2_5);\n",
    "    \\draw[->] (h1_3) -- (h2_5);\n",
    "    \\draw[->] (h1_4) -- (h2_5);\n",
    "    \\draw[->] (h1_5) -- (h2_5);\n",
    "    \\draw[->] (h1_6) -- (h2_5);\n",
    "    \\draw[->] (h1_0) -- (h2_6);\n",
    "    \\draw[->] (h1_1) -- (h2_6);\n",
    "    \\draw[->] (h1_2) -- (h2_6);\n",
    "    \\draw[->] (h1_3) -- (h2_6);\n",
    "    \\draw[->] (h1_4) -- (h2_6);\n",
    "    \\draw[->] (h1_5) -- (h2_6);\n",
    "    \\draw[->] (h1_6) -- (h2_6);\n",
    "\n",
    "    \\draw[->] (h2_0) -- (out_0);\n",
    "    \\draw[->] (h2_1) -- (out_0);\n",
    "    \\draw[->] (h2_2) -- (out_0);\n",
    "    \\draw[->] (h2_3) -- (out_0);\n",
    "    \\draw[->] (h2_4) -- (out_0);\n",
    "    \\draw[->] (h2_5) -- (out_0);\n",
    "    \\draw[->] (h2_6) -- (out_0);\n",
    "\n",
    "\n",
    "    \\node[fill=white] (beta0) at  (1.5, 2) {$\\mathbf{w^{(0)}}$};\n",
    "    \\node[fill=white] (beta1) at  (4.5, 2) {$\\mathbf{w^{(1)}}$};\n",
    "    \\node[fill=white] (beta2) at  (7.5, 2) {$\\mathbf{w^{(2)}}$};\n",
    "```\n",
    "\n",
    "The above graphical representation corresponds to the following model:\n",
    "\n",
    "\\begin{align}\n",
    "  \\hat{y} &= \\varphi \\left( \\sum_i w^{(2)}_{i} h^{(2)}_{i} + b^{(2)} \\right) \\\\\n",
    "  \\forall i, h^{(2)}_{i} &= \\varphi \\left( \\sum_j w^{(1)}_{ij} h^{(1)}_{j} + b^{(1)}_{i} \\right) \\\\\n",
    "  \\forall i, h^{(1)}_{i} &= \\varphi \\left( \\sum_j w^{(0)}_{ij} x_{j} + b^{(0)}_{i} \\right)\n",
    "\\end{align}\n",
    "\n",
    "To be even more precise, the bias terms $b^{(l)}_i$ are not represented in the graphical representation above.\n",
    "\n",
    "Such models with one or more hidden layers are called **Multi Layer Perceptrons** and we will present their characteristics in the following.\n",
    "\n",
    "\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "## The special case of the output layer\n",
    "\n",
    "## Declaring an MLP in `keras`\n",
    "\n",
    "In order to define a MLP model in `keras`, one just has to stack layers.\n",
    "As an example, if one wants to code a model made of:\n",
    "* an input layer with 10 neurons,\n",
    "* a hidden layer made of 20 neurons with ReLU activation,\n",
    "* an output layer made of 3 neurons with softmax activation, \n",
    "\n",
    "the code will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc857b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 16:45:35.572941: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-02 16:45:36.205921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-02 16:45:36.205964: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 16:45:36.249684: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 16:45:37.411167: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-02 16:45:37.411302: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-02 16:45:37.411315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense (Dense)               (None, 20)                220       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dense_1 (Dense)             (None, 3)                 63        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable params: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 16:45:39.408046: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-02 16:45:39.408093: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-02 16:45:39.408123: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fv-az489-299): /proc/driver/nvidia/version does not exist\n",
      "2022-10-02 16:45:39.409522: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    InputLayer(input_shape=(10, )),\n",
    "    Dense(units=20, activation=\"relu\"),\n",
    "    Dense(units=3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20cb0b",
   "metadata": {},
   "source": [
    "Note that `model.summary()` provides an interesting overview of a defined model and its parameters.\n",
    "\n",
    "````{admonition} Exercise\n",
    "\n",
    "Relying on what we have seen in this chapter, can you explain the number of parameters returned by `model.summary()` above?\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: dropdown, tip\n",
    "\n",
    "Our input layer is made of 10 neurons, and our first layer is fully connected, hence each of these neurons is connected to a neuron in the hidden layer through a parameter, which already makes $10 \\times 20 = 200$ parameters.\n",
    "Moreover, each of the hidden layer neurons has its own bias parameter, which is $20$ more parameters.\n",
    "We then have 220 parameters, as output by `model.summary()` for the layer `\"dense (Dense)\"`.\n",
    "\n",
    "Similarly, for the connection of the hidden layer neurons to those in the output layer, the total number of parameters is $20 \\times 3 = 60$ for the weights plus $3$ extra parameters for the biases.\n",
    "\n",
    "Overall, we have $220 + 63 = 283$ parameters in this model.\n",
    "```\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "source_map": [
   14,
   267,
   279
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}