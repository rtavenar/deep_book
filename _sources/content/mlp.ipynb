{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f136a5ae",
   "metadata": {},
   "source": [
    "(sec:mlp)=\n",
    "# Multi Layer Perceptrons (MLP)\n",
    "\n",
    "**TODO intro**\n",
    "\n",
    "## Why stacking layers?\n",
    "\n",
    "In the previous chapter, we have seen a very simple model called the Perceptron.\n",
    "In this model, the predicted output $\\hat{y}$ is computed as a linear combination of the input features plus a bias:\n",
    "\n",
    "$$\\hat{y} = \\sum_{j=1}^d x_j w_j + b$$\n",
    "\n",
    "In other words, we were optimizing among the family of linear models, which is a quite restricted family.\n",
    "In order to cover a wider range of models, one can stack neurons organized in layers to form a more complex model, such as the model below, which is called a one-hidden-layer-model, since an extra layer of neurons is introduced between the inputs and the output:\n",
    "\n",
    "```{tikz}\n",
    "    \\node[text width=3cm, align=center] (in_title) at  (0, 6) {Input layer\\\\ $\\mathbf{x}$};\n",
    "    \\node[text width=3cm, align=center] (h1_title) at  (3, 6) {Hidden layer 1\\\\ $\\mathbf{h^{(1)}}$};\n",
    "    \\node[text width=3cm, align=center] (out_title) at  (6, 6) {Output layer\\\\ $\\mathbf{\\hat{y}}$};\n",
    "\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in0) at  (0, 4) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in1) at  (0, 3) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in2) at  (0, 2) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in3) at  (0, 1) {};\n",
    "    \\node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in4) at  (0, 0) {};\n",
    "\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_0) at  (3, 5) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_1) at  (3, 4) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_2) at  (3, 3) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_3) at  (3, 2) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_4) at  (3, 1) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_5) at  (3, 0) {};\n",
    "    \\node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_6) at  (3, -1) {};\n",
    "    \n",
    "    \\node[draw, circle, fill=green, minimum size=17pt,inner sep=0pt] (out_0) at  (6, 2) {};\n",
    "    \\draw[->] (in0) -- (h1_0);\n",
    "    \\draw[->] (in0) -- (h1_1);\n",
    "    \\draw[->] (in0) -- (h1_2);\n",
    "    \\draw[->] (in0) -- (h1_3);\n",
    "    \\draw[->] (in0) -- (h1_4);\n",
    "    \\draw[->] (in0) -- (h1_5);\n",
    "    \\draw[->] (in0) -- (h1_6);\n",
    "    \\draw[->] (in1) -- (h1_0);\n",
    "    \\draw[->] (in1) -- (h1_1);\n",
    "    \\draw[->] (in1) -- (h1_2);\n",
    "    \\draw[->] (in1) -- (h1_3);\n",
    "    \\draw[->] (in1) -- (h1_4);\n",
    "    \\draw[->] (in1) -- (h1_5);\n",
    "    \\draw[->] (in1) -- (h1_6);\n",
    "    \\draw[->] (in2) -- (h1_0);\n",
    "    \\draw[->] (in2) -- (h1_1);\n",
    "    \\draw[->] (in2) -- (h1_2);\n",
    "    \\draw[->] (in2) -- (h1_3);\n",
    "    \\draw[->] (in2) -- (h1_4);\n",
    "    \\draw[->] (in2) -- (h1_5);\n",
    "    \\draw[->] (in2) -- (h1_6);\n",
    "    \\draw[->] (in3) -- (h1_0);\n",
    "    \\draw[->] (in3) -- (h1_1);\n",
    "    \\draw[->] (in3) -- (h1_2);\n",
    "    \\draw[->] (in3) -- (h1_3);\n",
    "    \\draw[->] (in3) -- (h1_4);\n",
    "    \\draw[->] (in3) -- (h1_5);\n",
    "    \\draw[->] (in3) -- (h1_6);\n",
    "    \\draw[->] (in4) -- (h1_0);\n",
    "    \\draw[->] (in4) -- (h1_1);\n",
    "    \\draw[->] (in4) -- (h1_2);\n",
    "    \\draw[->] (in4) -- (h1_3);\n",
    "    \\draw[->] (in4) -- (h1_4);\n",
    "    \\draw[->] (in4) -- (h1_5);\n",
    "    \\draw[->] (in4) -- (h1_6);\n",
    "    \\draw[->] (h1_0) -- (out_0);\n",
    "    \\draw[->] (h1_1) -- (out_0);\n",
    "    \\draw[->] (h1_2) -- (out_0);\n",
    "    \\draw[->] (h1_3) -- (out_0);\n",
    "    \\draw[->] (h1_5) -- (out_0);\n",
    "    \\draw[->] (h1_6) -- (out_0);\n",
    "\n",
    "\n",
    "    \\node[fill=white] (beta0) at  (1.5, 2) {$\\mathbf{w^{(0)}}$};\n",
    "    \\node[fill=white] (beta1) at  (4.5, 2) {$\\mathbf{w^{(1)}}$};\n",
    "```\n",
    "\n",
    "```{admonition} Universal Approximation Theorem\n",
    "\n",
    "The Universal Approximation Theorem states that any continuous function defined on a compact set can be \n",
    "approximated as closely as one wants by a one-hidden-layer neural network with sigmoid activation.\n",
    "```\n",
    "\n",
    "This Universal Approximation Theorem shows that by using a hidden layer to map inputs to outputs, one can now approximate any continuous function, which is a very interesting property.\n",
    "\n",
    "\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "## The special case of the output layer\n",
    "\n",
    "## Declaring an MLP in `keras`\n",
    "\n",
    "In order to define a MLP model in `keras`, one just has to stack layers.\n",
    "As an example, if one wants to code a model made of:\n",
    "* an input layer with 10 neurons,\n",
    "* a hidden layer made of 20 neurons with ReLU activation,\n",
    "* an output layer made of 3 neurons with softmax activation, \n",
    "\n",
    "the code will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5240f9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, InputLayer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m      5\u001b[0m     InputLayer(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, )),\n\u001b[1;32m      6\u001b[0m     Dense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     Dense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m ])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    InputLayer(input_shape=(10, )),\n",
    "    Dense(units=20, activation=\"relu\"),\n",
    "    Dense(units=3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4303f7",
   "metadata": {},
   "source": [
    "Note that `model.summary()` provides an interesting overview of a defined model and its parameters.\n",
    "\n",
    "````{admonition} Exercise\n",
    "\n",
    "Relying on what we have seen in this chapter, can you explain the number of parameters returned by `model.summary()` above?\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: dropdown, tip\n",
    "\n",
    "Our input layer is made of 10 neurons, and our first layer is fully connected, hence each of these neurons is connected to a neuron in the hidden layer through a parameter, which already makes $10 \\times 20 = 200$ parameters.\n",
    "Moreover, each of the hidden layer neurons has its own bias parameter, which is $20$ more parameters.\n",
    "We then have 220 parameters, as output by `model.summary()` for the layer `\"dense (Dense)\"`.\n",
    "\n",
    "Similarly, for the connection of the hidden layer neurons to those in the output layer, the total number of parameters is $20 \\times 3 = 60$ for the weights plus $3$ extra parameters for the biases.\n",
    "\n",
    "Overall, we have $220 + 63 = 283$ parameters in this model.\n",
    "```\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "source_map": [
   14,
   122,
   134
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}