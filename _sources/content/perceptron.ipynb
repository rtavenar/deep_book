{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e339ed16",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this introduction chapter, we will present a first neural network called\n",
    "the Perceptron.\n",
    "This model is a neural network made of a single neuron, and we will use it here as a way to introduce key concepts that we will detail later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efabaf0e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnotebook_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_notebook_graphics\n\u001b[1;32m      6\u001b[0m prepare_notebook_graphics()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from notebook_utils import prepare_notebook_graphics\n",
    "prepare_notebook_graphics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e014591",
   "metadata": {},
   "source": [
    "## A first model: the Perceptron\n",
    "\n",
    "In the neural network terminology, a neuron is a parametrized function that\n",
    "takes a vector $\\mathbf{x}$ as input and outputs a single value $a$ as follows:\n",
    "\n",
    "$$\n",
    "    a = \\varphi(\\underbrace{\\mathbf{w} \\mathbf{x} + b}_{o}) ,\n",
    "$$\n",
    "\n",
    "where the parameters of the neuron are its weights stored in $\\mathbf{w}$\n",
    "and a bias term $b$, and $\\varphi$ is an activation function that is chosen\n",
    "_a priori_\n",
    "(we will come back to it in more details later in the course):\n",
    "\n",
    "```{tikz}\n",
    "    \\node[draw,circle,minimum size=25pt,inner sep=0pt] (x) at (0,0) {$o$};\n",
    "    \\node[draw,circle,fill=green,fill opacity=.2,text opacity=1.,minimum size=25pt,inner sep=0pt] (a) at (2, 0) {$\\tiny a$};\n",
    "\n",
    "\t\\node[draw,circle,fill=blue,fill opacity=.2,text opacity=1.,minimum size=25pt,inner sep=0pt] (x0) at (-2, 2) {$\\tiny x_0$};\n",
    "\t\\node[draw,circle,fill=blue,fill opacity=.2,text opacity=1.,minimum size=25pt,inner sep=0pt] (x1) at (-2, 1) {$\\tiny x_1$};\n",
    "\t\\node[draw,circle,fill=blue,fill opacity=.2,text opacity=1.,minimum size=25pt,inner sep=0pt] (x2) at (-2, 0) {$\\tiny x_2$};\n",
    "\t\\node[draw,circle,fill=blue,fill opacity=.2,text opacity=1.,minimum size=25pt,inner sep=0pt] (x3) at (-2, -1) {$\\tiny x_3$};\n",
    "\t\\node[draw,circle,fill=blue,fill opacity=.2,text opacity=1.,minimum size=25pt,inner sep=0pt] (b) at (-2, -2) {$\\tiny +1$};\n",
    "\n",
    "\t\\draw[->, thick] (x0) to[out=0,in=120] node [midway, sloped, above=-2] {$w_0$} (x);\n",
    "\t\\draw[->, thick] (x1) to[out=0,in=150] node [midway, sloped, above=-2] {$w_1$} (x);\n",
    "\t\\draw[->, thick] (x2) to[out=0,in=180] node [midway, sloped, above=-2] {$w_2$} (x);\n",
    "\t\\draw[->, thick] (x3) to[out=0,in=210] node [midway, sloped, above=-2] {$w_3$} (x);\n",
    "\t\\draw[->, thick] (b) to[out=0,in=240] node [midway, sloped, above=-2] {$b$} (x);\n",
    "\n",
    "\t\\draw[->, thick] (x) to node [midway,above=-0.1cm] {$\\varphi$} (a);\n",
    "```\n",
    "\n",
    "A model made of a single neuron is called a Perceptron.\n",
    "\n",
    "## Optimization\n",
    "\n",
    "The models presented in this book are aimed at solving prediction problems, in\n",
    "which the goal is to find \"good enough\" parameter values for the model at stake\n",
    "given some observed data.\n",
    "\n",
    "The problem of finding such parameter values is coined optimization and the deep\n",
    "learning field makes extensive use of a specific family of optimization\n",
    "strategies called **gradient descent**.\n",
    "\n",
    "(sec:boston)=\n",
    "### Gradient Descent\n",
    "\n",
    "To make one's mind about gradient descent, let us assume we are given\n",
    "the following dataset about house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "boston = pd.read_csv(\"data/boston.csv\")[[\"RM\", \"PRICE\"]]\n",
    "boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9126bd",
   "metadata": {},
   "source": [
    "In our case, we will try (for a start) to predict the target value of this\n",
    "dataset, which is the median value of owner-occupied homes in $1000\n",
    "`\"PRICE\"`, as a function of the average number of rooms per dwelling `\"RM\"` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62836a",
   "metadata": {
    "render": {
     "image": {
      "tex_specific_width": "60%"
     }
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=boston, x=\"RM\", y=\"PRICE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3bb97",
   "metadata": {},
   "source": [
    "```{sidebar} A short note on this model\n",
    "\n",
    "In the Perceptron terminology, this model:\n",
    "* has no activation function (_i.e._ $\\varphi$ is the identity function)\n",
    "* has no bias (_i.e._ $b$ is forced to be $0$, it is not learnt)\n",
    "```\n",
    "\n",
    "Let us assume we have a naive approach in which our prediction model is linear\n",
    "without intercept, that is, for a given input $x_i$ the predicted output is\n",
    "computed as:\n",
    "\n",
    "$$\n",
    "    \\hat{y_i} = w x_i\n",
    "$$\n",
    "\n",
    "where $w$ is the only parameter of our model.\n",
    "\n",
    "Let us further assume that the quantity we aim at minimizing\n",
    "(our objective, also called loss) is:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(w) = \\sum_i \\left(\\hat{y_i} - y_i\\right)^2\n",
    "$$\n",
    "\n",
    "where $y_i$ is the ground truth value associated with the $i$-th sample in our\n",
    "dataset.\n",
    "\n",
    "Let us have a look at this quantity as a function of $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da5698",
   "metadata": {
    "render": {
     "image": {
      "tex_specific_width": "60%"
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(w, x, y):\n",
    "    w = np.array(w)\n",
    "    return np.sum(\n",
    "        (w[:, None] * x.to_numpy()[None, :] - y.to_numpy()[None, :]) ** 2,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "w = np.linspace(-2, 10, num=100)\n",
    "\n",
    "x = boston[\"RM\"]\n",
    "y = boston[\"PRICE\"]\n",
    "plt.plot(w, loss(w, x, y), \"r-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f96ced",
   "metadata": {},
   "source": [
    "Here, it seems that a value of $w$ around 4 should be a good pick, but this\n",
    "method (generating lots of values for the parameter and computing the loss for\n",
    "each value) cannot scale to models that have lots of parameters, so we will\n",
    "try something else.\n",
    "\n",
    "Let us suppose we have access, each time we pick a candidate value for $w$,\n",
    "to both the loss $\\mathcal{L}$ and information about how $\\mathcal{L}$ varies,\n",
    "locally.\n",
    "We could, in this case, compute a new candidate value for $w$ by moving from\n",
    "the previous candidate value in the direction of steepest descent.\n",
    "This is the basic idea behind the gradient descent algorithm that, from an\n",
    "initial candidate $w_0$, iteratively computes new candidates as:\n",
    "\n",
    "$$\n",
    "    w_{t+1} = w_t - \\rho \\left. \\frac{\\partial \\mathcal{L}}{\\partial w} \\right|_{w=w_t}\n",
    "$$\n",
    "\n",
    "where $\\rho$ is a hyper-parameter (called the learning rate)\n",
    "that controls the size of the steps to be done, and\n",
    "$\\left. \\frac{\\partial \\mathcal{L}}{\\partial w} \\right|_{w=w_t}$ is the\n",
    "gradient of\n",
    "$\\mathcal{L}$ with respect to $w$, evaluated at $w=w_t$.\n",
    "As you can see, the direction of steepest descent is the opposite of the\n",
    "direction pointed by the gradient (and this holds when dealing with vector\n",
    "parameters too).\n",
    "\n",
    "This process is repeated until convergence, as illustrated in the following\n",
    "visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300af568",
   "metadata": {
    "render": {
     "image": {
      "tex_specific_width": "60%"
     }
    }
   },
   "outputs": [],
   "source": [
    "rho = 1e-5\n",
    "\n",
    "def grad_loss(w_t, x, y):\n",
    "    return np.sum(\n",
    "        2 * (w_t * x - y) * x\n",
    "    )\n",
    "\n",
    "\n",
    "ww = np.linspace(-2, 10, num=100)\n",
    "plt.plot(ww, loss(ww, x, y), \"r-\", alpha=.5);\n",
    "\n",
    "w = [0.]\n",
    "for t in range(10):\n",
    "    w_update = w[t] - rho * grad_loss(w[t], x, y)\n",
    "    w.append(w_update)\n",
    "\n",
    "plt.plot(w, loss(w, x, y), \"ko-\")\n",
    "plt.text(x=w[0]+.1, y=loss([w[0]], x, y), s=\"$w_{0}$\")\n",
    "plt.text(x=w[10]+.1, y=loss([w[10]], x, y), s=\"$w_{10}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632bb8b",
   "metadata": {},
   "source": [
    "What would we get if we used a smaller learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a580714",
   "metadata": {
    "render": {
     "image": {
      "tex_specific_width": "60%"
     }
    }
   },
   "outputs": [],
   "source": [
    "rho = 1e-6\n",
    "\n",
    "ww = np.linspace(-2, 10, num=100)\n",
    "plt.plot(ww, loss(ww, x, y), \"r-\", alpha=.5);\n",
    "\n",
    "w = [0.]\n",
    "for t in range(10):\n",
    "    w_update = w[t] - rho * grad_loss(w[t], x, y)\n",
    "    w.append(w_update)\n",
    "\n",
    "plt.plot(w, loss(w, x, y), \"ko-\")\n",
    "plt.text(x=w[0]+.1, y=loss([w[0]], x, y), s=\"$w_{0}$\")\n",
    "plt.text(x=w[10]+.1, y=loss([w[10]], x, y), s=\"$w_{10}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58e849",
   "metadata": {},
   "source": [
    "It would definitely take more time to converge.\n",
    "But, take care, a larger learning rate is not always a good idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c7f78",
   "metadata": {
    "render": {
     "image": {
      "tex_specific_width": "60%"
     }
    }
   },
   "outputs": [],
   "source": [
    "rho = 5e-5\n",
    "\n",
    "ww = np.linspace(-2, 10, num=100)\n",
    "plt.plot(ww, loss(ww, x, y), \"r-\", alpha=.5);\n",
    "\n",
    "w = [0.]\n",
    "for t in range(10):\n",
    "    w_update = w[t] - rho * grad_loss(w[t], x, y)\n",
    "    w.append(w_update)\n",
    "\n",
    "plt.plot(w, loss(w, x, y), \"ko-\")\n",
    "plt.text(x=w[0]-1., y=loss([w[0]], x, y), s=\"$w_{0}$\")\n",
    "plt.text(x=w[10]-1., y=loss([w[10]], x, y), s=\"$w_{10}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ac67e",
   "metadata": {},
   "source": [
    "See how we are slowly diverging because our steps are too large?\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this section, we have introduced:\n",
    "* a very simple model, called the Perceptron: this will be a building block for the more advanced models we will detail later in the course, such as:\n",
    "    * the [Multi-Layer Perceptron](sec:mlp)\n",
    "    * [Convolutional architectures](sec:cnn)\n",
    "* the fact that a task comes with a loss function to be minimized (here, we have used the _mean squared error (MSE)_ for our regression task), which will be discussed in [a dedicated chapter](sec:loss);\n",
    "* the concept of gradient descent to optimize the chosen loss over a model's single parameter, and this will be extended in [our chapter on optimization](sec:sgd)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   14,
   22,
   31,
   84,
   89,
   95,
   102,
   133,
   153,
   184,
   209,
   213,
   232,
   237,
   256
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}