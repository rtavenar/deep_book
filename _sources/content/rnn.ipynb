{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0e6343",
   "metadata": {},
   "source": [
    "(sec:rnn)=\n",
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks (RNNs) proceed by processing elements of a time\n",
    "series one at a time.\n",
    "Typically, at time $t$, a recurrent block will take both the current input $x_t$\n",
    "and a hidden state $h_{t-1}$ that aims at summarizing the key information from\n",
    "past inputs $\\{x_0, \\dots, x_{t-1}\\}$, and will output an updated hidden state\n",
    "$h_{t}$.\n",
    "There exist various recurrent modules that mostly differ in the way $h_t$ is\n",
    "computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b42c9e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4900b7e3",
   "metadata": {},
   "source": [
    "## \"Vanilla\" RNNs\n",
    "\n",
    "The basic formulation for a RNN block is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\forall t, h_t = \\text{tanh}(W_h h_{t-1} + W_x x_t + b)\n",
    "\\end{equation}\n",
    "\n",
    "where $W_h$ is a weight matrix associated to the processing of the previous\n",
    "hidden state, $W_x$ is another weight matrix associated to the processing of\n",
    "the current input and $b$ is a bias term.\n",
    "\n",
    "Note here that $W_h$, $W_x$ and $b$ are not indexed by $t$, which means that\n",
    "they are **shared across all timestamps**.\n",
    "\n",
    "An important limitation of this formula is that it easily fails at capturing\n",
    "long-term dependencies.\n",
    "To better understand why, one should remind that the parameters of these\n",
    "networks are optimized through stochastic gradient descent algorithms.\n",
    "\n",
    "To simplify notations, let us consider a simplified case in which\n",
    "$h_t$ and $x_t$ are both scalar values, and let us have a look at what the\n",
    "actual gradient of the output $h_t$ is, with\n",
    "respect to $W_h$ (which is then also a scalar):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{W_h}(h_t) = \\text{tanh}^\\prime(o_t) \\cdot \\frac{\\partial o_t}{\\partial W_h}\n",
    "\\end{equation}\n",
    "\n",
    "where $o_t = W_h h_{t-1} + W_x x_t + b$, hence:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial o_t}{\\partial W_h} = h_{t-1} + W_h \\cdot \\frac{\\partial h_{t-1}}{\\partial W_h} \\, .\n",
    "\\end{equation}\n",
    "\n",
    "Here, the form of $\\frac{\\partial h_{t-1}}{\\partial W_h}$ will be similar to\n",
    "that of $\\nabla_{W_h}(h_t)$ above, and, in the end, one gets:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    \\nabla_{W_h}(h_t) &=& \\text{tanh}^\\prime(o_t) \\cdot\n",
    "        \\left[\n",
    "            h_{t-1} + W_h \\cdot \\frac{\\partial h_{t-1}}{\\partial W_h}\n",
    "        \\right] \\\\\n",
    "        &=& \\text{tanh}^\\prime(o_t) \\cdot\n",
    "           \\left[\n",
    "               h_{t-1} + W_h \\cdot \\text{tanh}^\\prime(o_{t-1}) \\cdot\n",
    "               \\left[\n",
    "                   h_{t-2} + W_h \\cdot \\left[ \\dots \\right]\n",
    "               \\right]\n",
    "           \\right] \\\\\n",
    "          &=& h_{t-1} \\text{tanh}^\\prime(o_t) + h_{t-2} W_h \\text{tanh}^\\prime(o_t) \\text{tanh}^\\prime(o_{t-1}) + \\dots \\\\\n",
    "         &=& \\sum_{t^\\prime = 1}^{t-1} h_{t^\\prime} \\left[ W_h^{t-t^\\prime-1} \\text{tanh}^\\prime(o_{t^\\prime+1}) \\cdot \\cdots \\cdot  \\text{tanh}^\\prime(o_{t}) \\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "In other words, the influence of $h_{t^\\prime}$ will be mitigated by a factor\n",
    "$W_h^{t-t^\\prime-1} \\text{tanh}^\\prime(o_{t^\\prime+1}) \\cdot \\cdots \\cdot  \\text{tanh}^\\prime(o_{t})$.\n",
    "\n",
    "Now recall what the tanh function and its derivative look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbbeabe",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtanh\u001b[39m(x):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m2.\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tanh(x):\n",
    "    return 2. / (1. + tf.exp(-2 * x)) - 1.\n",
    "\n",
    "x = tf.Variable(tf.linspace(-4, 4, 50))\n",
    "with tf.GradientTape() as tape:\n",
    "    tan_x = tanh(x)\n",
    "grad_tanh_x = tape.gradient(tan_x, x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x.numpy(), tan_x.numpy(), label='tanh(x)')\n",
    "plt.plot(x.numpy(), grad_tanh_x, label='tanh\\'(x)')\n",
    "plt.legend()\n",
    "plt.grid('on');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab830b16",
   "metadata": {},
   "source": [
    "One can see how quickly gradients gets close to 0 for inputs larger\n",
    "(in absolute value) than 2, and having multiple such terms in a\n",
    "computation chain will likely make the corresponding terms vanish.\n",
    "\n",
    "In other words, the gradient of the hidden state at time $t$ will only be\n",
    "influenced by a few of its predecessors $\\{h_{t-1}, h_{t-2}, \\dots\\}$ and\n",
    "long-term dependencies will be ignored when updating model parameters through\n",
    "gradient descent.\n",
    "This is an occurrence of a more general phenomenon known as the\n",
    "**vanishing gradient** effect.\n",
    "\n",
    "## Long Short-Term Memory\n",
    "\n",
    "The Long Short-Term Memory (LSTM, {cite:p}`hochreiter1997long`) blocks have\n",
    "been designed as an alternative\n",
    "recurrent block that aims at mitigating this vanishing gradient effect through\n",
    "the use of gates that explicitly encode pieces of information that should\n",
    "(resp. should not) be kept in computations.\n",
    "\n",
    "```{admonition} Gates in neural networks\n",
    ":class: tip\n",
    "\n",
    "In the neural networks terminology, a gate $g \\in [0, 1]^d$ is a vector that is\n",
    "used to filter out information from an incoming feature vector\n",
    "$v \\in \\mathbb{R}^d$ such that the result of applying the gate is: $g \\odot v$\n",
    "where $\\odot$ is the element-wise product.\n",
    "The gate $g$ will hence tend to remove part of the features in $v$\n",
    "(those corresponding to very low values in $g$).\n",
    "```\n",
    "\n",
    "In these blocks, an extra state is used, referred to as the cell state $C_t$.\n",
    "This state is computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "\\end{equation}\n",
    "\n",
    "where $f_t$ is the forget gate (which pushes the network to forget about\n",
    "useless parts of the past cell state),\n",
    "$i_t$ is the input gate and $\\tilde{C}_t$ is\n",
    "an updated version of the cell state (which, in turn, can be partly censored\n",
    "by the input gate).\n",
    "\n",
    "Let us delay for now the details about how these 3 terms are computed, and\n",
    "rather focus on how the formula above is significantly different from the\n",
    "update rule of the hidden state in vanilla RNNs.\n",
    "Indeed, in this case, if the network learns so (through $f_t$), the\n",
    "full information from the previous cell state $C_{t-1}$ can be recovered,\n",
    "which would allow gradients to flow through time (and not vanish anymore).\n",
    "\n",
    "Then, the link between the cell and hidden states is:\n",
    "\n",
    "\\begin{equation}\n",
    "    h_t = o_t \\odot \\text{tanh}(C_{t}) \\, .\n",
    "\\end{equation}\n",
    "\n",
    "In words, the hidden state is the tanh-transformed version of the cell state,\n",
    "further censored by an output gate $o_t$.\n",
    "\n",
    "All gates used in the formulas above are defined similarly:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    f_t &=& \\sigma ( W_f \\cdot [h_{t-1}, x_t] + b_f) \\\\\n",
    "    i_t &=& \\sigma ( W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\\n",
    "    o_t &=& \\sigma ( W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\sigma$ is the sigmoid activation function\n",
    "(which has values in $[0, 1]$) and $[h_{t-1}, x_t]$ is\n",
    "the concatenation of $h_{t-1}$ and $x_t$ features.\n",
    "\n",
    "Finally, the updated cell state $\\tilde{C}_t$ is computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{C}_t = \\text{tanh}(W_C \\cdot [h_{t-1}, x_t] + b_C) \\, .\n",
    "\\end{equation}\n",
    "\n",
    "Many variants over these LSTM blocks exist in the literature that still rely\n",
    "on the same basic principles.\n",
    "\n",
    "## Gated Recurrent Unit\n",
    "\n",
    "A slightly different parametrization of a recurrent block is used in the\n",
    "so-called Gatted Recurrent Unit (GRU, {cite:p}`cho2014properties`).\n",
    "\n",
    "GRUs also rely on the use of gates to (adaptively) let information flow\n",
    "through time.\n",
    "A first significant difference between GRUs and LSTMs, though, is that GRUs\n",
    "do not resort to the use of a cell state.\n",
    "Instead, the update rule for the hidden state is:\n",
    "\n",
    "\\begin{equation}\n",
    "    h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "\\end{equation}\n",
    "\n",
    "where $z_t$ is a gate that balances (per feature) the amount of information\n",
    "that is kept from the previous hidden state with the amount of information\n",
    "that should be updated using the new candidate hidden state $\\tilde{h}_t$,\n",
    "computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{h}_t = \\text{tanh}(W \\cdot [r_t \\odot h_{t-1}, x_t] + b) \\, ,\n",
    "\\end{equation}\n",
    "\n",
    "where $r_t$ is an extra gate that can hide part of the previous hidden state.\n",
    "\n",
    "Formulas for gates $z_t$ and $r_t$ are similar to those provided for $f_t$,\n",
    "$i_t$ and $o_t$ in the case of LSTMs.\n",
    "\n",
    "A study of the ability of these variants of recurrent networks to learn\n",
    "long-term dependencies is provided\n",
    "[in this online publication](https://distill.pub/2019/memorization-in-rnns/).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this chapter, we have reviewed neural network architectures that are\n",
    "used to learn from time series datasets.\n",
    "Because of time constraints, we have not tackled attention-based models\n",
    "in this course.\n",
    "We have presented convolutional models that aim at extracting discriminative\n",
    "local shapes in the series and recurrent models that rather leverage the\n",
    "notion of sequence.\n",
    "Concerning the latter, variants that aim at facing the vanishing gradient\n",
    "effect have been introduced.\n",
    "Note that recurrent models are known to require more training data than\n",
    "their convolutional counterparts in order to\n",
    "learn meaningful representations.\n",
    "\n",
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "source_map": [
   14,
   28,
   36,
   97,
   115
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}