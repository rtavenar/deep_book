{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6b2b55",
   "metadata": {},
   "source": [
    "(sec:sgd)=\n",
    "# Optimization\n",
    "\n",
    "In this chapter, we will present an optimization strategy called **Gradient Descent** and its variants, and show how they can be used to optimize neural network parameters.\n",
    "\n",
    "**Coming soon**\n",
    "\n",
    "<!-- **TODO: Ici, illustrer non convexitÃ© ?**\n",
    "\n",
    "## SGD\n",
    "\n",
    "## Variants of SGD (towards Adam)\n",
    "\n",
    "## The curse of depth\n",
    "\n",
    "**TODO:** A first implication: use ReLU activation functions if you have no reason to use anything else. (illustrate this?)\n",
    "\n",
    "**TODO**: talk about feature standardization and how it eases the convergence to a good solution -->"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}