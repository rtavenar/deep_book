
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Multi Layer Perceptrons (MLP) &#8212; Deep Learning Basics (lecture notes)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Losses" href="loss.html" />
    <link rel="prev" title="Introduction" href="perceptron.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Deep Learning Basics (lecture notes)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Deep Learning Basics
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Multi Layer Perceptrons (MLP)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss.html">
   Losses
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optim.html">
   Optimization
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/rtavenar/deep_book/main?urlpath=tree/content/mlp.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/rtavenar/deep_book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/rtavenar/deep_book/issues/new?title=Issue%20on%20page%20%2Fcontent/mlp.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/content/mlp.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="../_sources/content/mlp.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacking-layers-for-better-expressivity">
   Stacking layers for better expressivity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deciding-on-an-mlp-architecture">
   Deciding on an MLP architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-special-case-of-the-output-layer">
     The special case of the output layer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#declaring-an-mlp-in-keras">
   Declaring an MLP in
   <code class="docutils literal notranslate">
    <span class="pre">
     keras
    </span>
   </code>
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Multi Layer Perceptrons (MLP)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stacking-layers-for-better-expressivity">
   Stacking layers for better expressivity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deciding-on-an-mlp-architecture">
   Deciding on an MLP architecture
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions">
   Activation functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-special-case-of-the-output-layer">
     The special case of the output layer
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#declaring-an-mlp-in-keras">
   Declaring an MLP in
   <code class="docutils literal notranslate">
    <span class="pre">
     keras
    </span>
   </code>
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="multi-layer-perceptrons-mlp">
<span id="sec-mlp"></span><h1>Multi Layer Perceptrons (MLP)<a class="headerlink" href="#multi-layer-perceptrons-mlp" title="Permalink to this headline">#</a></h1>
<p>In the previous chapter, we have seen a very simple model called the Perceptron.
In this model, the predicted output <span class="math notranslate nohighlight">\(\hat{y}\)</span> is computed as a linear combination of the input features plus a bias:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = \sum_{j=1}^d x_j w_j + b\]</div>
<p>In other words, we were optimizing among the family of linear models, which is a quite restricted family.</p>
<section id="stacking-layers-for-better-expressivity">
<h2>Stacking layers for better expressivity<a class="headerlink" href="#stacking-layers-for-better-expressivity" title="Permalink to this headline">#</a></h2>
<p>In order to cover a wider range of models, one can stack neurons organized in layers to form a more complex model, such as the model below, which is called a one-hidden-layer model, since an extra layer of neurons is introduced between the inputs and the output:</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">    \node[text width=3cm, align=center] (in_title) at  (0, 6) {Input layer\\ $\mathbf{x}$};
    \node[text width=3cm, align=center] (h1_title) at  (3, 6) {Hidden layer 1\\ $\mathbf{h^{(1)}}$};
    \node[text width=3cm, align=center] (out_title) at  (6, 6) {Output layer\\ $\mathbf{\hat{y}}$};

    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in0) at  (0, 4) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in1) at  (0, 3) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in2) at  (0, 2) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in3) at  (0, 1) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in4) at  (0, 0) {};

    \node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_0) at  (3, 5) {};
    \node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_1) at  (3, 4) {};
    \node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_2) at  (3, 3) {};
    \node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_3) at  (3, 2) {};
    \node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_4) at  (3, 1) {};
    \node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_5) at  (3, 0) {};
    \node[draw, circle, minimum size=17pt,inner sep=0pt] (h1_6) at  (3, -1) {};
    
    \node[draw, circle, fill=green, minimum size=17pt,inner sep=0pt] (out_0) at  (6, 2) {};
    \draw[-&gt;] (in0) -- (h1_0);
    \draw[-&gt;] (in0) -- (h1_1);
    \draw[-&gt;] (in0) -- (h1_2);
    \draw[-&gt;] (in0) -- (h1_3);
    \draw[-&gt;] (in0) -- (h1_4);
    \draw[-&gt;] (in0) -- (h1_5);
    \draw[-&gt;] (in0) -- (h1_6);
    \draw[-&gt;] (in1) -- (h1_0);
    \draw[-&gt;] (in1) -- (h1_1);
    \draw[-&gt;] (in1) -- (h1_2);
    \draw[-&gt;] (in1) -- (h1_3);
    \draw[-&gt;] (in1) -- (h1_4);
    \draw[-&gt;] (in1) -- (h1_5);
    \draw[-&gt;] (in1) -- (h1_6);
    \draw[-&gt;] (in2) -- (h1_0);
    \draw[-&gt;] (in2) -- (h1_1);
    \draw[-&gt;] (in2) -- (h1_2);
    \draw[-&gt;] (in2) -- (h1_3);
    \draw[-&gt;] (in2) -- (h1_4);
    \draw[-&gt;] (in2) -- (h1_5);
    \draw[-&gt;] (in2) -- (h1_6);
    \draw[-&gt;] (in3) -- (h1_0);
    \draw[-&gt;] (in3) -- (h1_1);
    \draw[-&gt;] (in3) -- (h1_2);
    \draw[-&gt;] (in3) -- (h1_3);
    \draw[-&gt;] (in3) -- (h1_4);
    \draw[-&gt;] (in3) -- (h1_5);
    \draw[-&gt;] (in3) -- (h1_6);
    \draw[-&gt;] (in4) -- (h1_0);
    \draw[-&gt;] (in4) -- (h1_1);
    \draw[-&gt;] (in4) -- (h1_2);
    \draw[-&gt;] (in4) -- (h1_3);
    \draw[-&gt;] (in4) -- (h1_4);
    \draw[-&gt;] (in4) -- (h1_5);
    \draw[-&gt;] (in4) -- (h1_6);
    \draw[-&gt;] (h1_0) -- (out_0);
    \draw[-&gt;] (h1_1) -- (out_0);
    \draw[-&gt;] (h1_2) -- (out_0);
    \draw[-&gt;] (h1_3) -- (out_0);
    \draw[-&gt;] (h1_4) -- (out_0);
    \draw[-&gt;] (h1_5) -- (out_0);
    \draw[-&gt;] (h1_6) -- (out_0);


    \node[fill=white] (beta0) at  (1.5, 2) {$\mathbf{w^{(0)}}$};
    \node[fill=white] (beta1) at  (4.5, 2) {$\mathbf{w^{(1)}}$};</span>)</p>
<p>!pdf2svg command cannot be run</p>
</div>
</div><p>The question one might ask now is whether this added hidden layer effectively allows to cover a wider family of models.
This is what the Universal Approximation Theorem below is all about.</p>
<div class="admonition-universal-approximation-theorem admonition">
<p class="admonition-title">Universal Approximation Theorem</p>
<p>The Universal Approximation Theorem states that any continuous function defined on a compact set can be
approximated as closely as one wants by a one-hidden-layer neural network with sigmoid activation.</p>
</div>
<p>In other words, by using a hidden layer to map inputs to outputs, one can now approximate any continuous function, which is a very interesting property.
Note however that the number of hidden neurons that is necessary to achieve a given approximation quality is not discussed here.
Moreover, it is not sufficient that such a good approximation exists, another important question is whether the optimization algorithms we will use will eventually converge to this solution or not, which is not guaranteed, as discussed in more details in <a class="reference internal" href="optim.html#sec-sgd"><span class="std std-ref">the dedicated chapter</span></a>.</p>
<p>In practice, we observe empirically that in order to achieve a given approximation quality, it is more efficient (in terms of the number of parameters required) to stack several hidden layers rather than rely on a single one :</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<span class="docutils literal">    \node[text width=3cm, align=center] (in_title) at  (0, 6) {Input layer\\ $\mathbf{x}$};
    \node[text width=3cm, align=center] (h1_title) at  (3, 6) {Hidden layer 1\\ $\mathbf{h^{(1)}}$};
    \node[text width=3cm, align=center] (h1_title) at  (6, 6) {Hidden layer 2\\ $\mathbf{h^{(2)}}$};
    \node[text width=3cm, align=center] (out_title) at  (9, 6) {Output layer\\ $\mathbf{\hat{y}}$};

    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in0) at  (0, 4) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in1) at  (0, 3) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in2) at  (0, 2) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in3) at  (0, 1) {};
    \node[draw, circle, fill=blue, minimum size=17pt,inner sep=0pt] (in4) at  (0, 0) {};

    \node[draw, circle, fill=cyan, minimum size=17pt,inner sep=0pt] (h1_0) at  (3, 5) {};
    \node[draw, circle, fill=cyan, minimum size=17pt,inner sep=0pt] (h1_1) at  (3, 4) {};
    \node[draw, circle, fill=cyan, minimum size=17pt,inner sep=0pt] (h1_2) at  (3, 3) {};
    \node[draw, circle, fill=cyan, minimum size=17pt,inner sep=0pt] (h1_3) at  (3, 2) {};
    \node[draw, circle, fill=cyan, minimum size=17pt,inner sep=0pt] (h1_4) at  (3, 1) {};
    \node[draw, circle, fill=cyan, minimum size=17pt,inner sep=0pt] (h1_5) at  (3, 0) {};
    \node[draw, circle, fill=cyan, minimum size=17pt,inner sep=0pt] (h1_6) at  (3, -1) {};

    \node[draw, circle, fill=teal, minimum size=17pt,inner sep=0pt] (h2_0) at  (6, 5) {};
    \node[draw, circle, fill=teal, minimum size=17pt,inner sep=0pt] (h2_1) at  (6, 4) {};
    \node[draw, circle, fill=teal, minimum size=17pt,inner sep=0pt] (h2_2) at  (6, 3) {};
    \node[draw, circle, fill=teal, minimum size=17pt,inner sep=0pt] (h2_3) at  (6, 2) {};
    \node[draw, circle, fill=teal, minimum size=17pt,inner sep=0pt] (h2_4) at  (6, 1) {};
    \node[draw, circle, fill=teal, minimum size=17pt,inner sep=0pt] (h2_5) at  (6, 0) {};
    \node[draw, circle, fill=teal, minimum size=17pt,inner sep=0pt] (h2_6) at  (6, -1) {};
    
    \node[draw, circle, fill=green, minimum size=17pt,inner sep=0pt] (out_0) at  (9, 2) {};
    \draw[-&gt;] (in0) -- (h1_0);
    \draw[-&gt;] (in0) -- (h1_1);
    \draw[-&gt;] (in0) -- (h1_2);
    \draw[-&gt;] (in0) -- (h1_3);
    \draw[-&gt;] (in0) -- (h1_4);
    \draw[-&gt;] (in0) -- (h1_5);
    \draw[-&gt;] (in0) -- (h1_6);
    \draw[-&gt;] (in1) -- (h1_0);
    \draw[-&gt;] (in1) -- (h1_1);
    \draw[-&gt;] (in1) -- (h1_2);
    \draw[-&gt;] (in1) -- (h1_3);
    \draw[-&gt;] (in1) -- (h1_4);
    \draw[-&gt;] (in1) -- (h1_5);
    \draw[-&gt;] (in1) -- (h1_6);
    \draw[-&gt;] (in2) -- (h1_0);
    \draw[-&gt;] (in2) -- (h1_1);
    \draw[-&gt;] (in2) -- (h1_2);
    \draw[-&gt;] (in2) -- (h1_3);
    \draw[-&gt;] (in2) -- (h1_4);
    \draw[-&gt;] (in2) -- (h1_5);
    \draw[-&gt;] (in2) -- (h1_6);
    \draw[-&gt;] (in3) -- (h1_0);
    \draw[-&gt;] (in3) -- (h1_1);
    \draw[-&gt;] (in3) -- (h1_2);
    \draw[-&gt;] (in3) -- (h1_3);
    \draw[-&gt;] (in3) -- (h1_4);
    \draw[-&gt;] (in3) -- (h1_5);
    \draw[-&gt;] (in3) -- (h1_6);
    \draw[-&gt;] (in4) -- (h1_0);
    \draw[-&gt;] (in4) -- (h1_1);
    \draw[-&gt;] (in4) -- (h1_2);
    \draw[-&gt;] (in4) -- (h1_3);
    \draw[-&gt;] (in4) -- (h1_4);
    \draw[-&gt;] (in4) -- (h1_5);
    \draw[-&gt;] (in4) -- (h1_6);

    \draw[-&gt;] (h1_0) -- (h2_0);
    \draw[-&gt;] (h1_1) -- (h2_0);
    \draw[-&gt;] (h1_2) -- (h2_0);
    \draw[-&gt;] (h1_3) -- (h2_0);
    \draw[-&gt;] (h1_4) -- (h2_0);
    \draw[-&gt;] (h1_5) -- (h2_0);
    \draw[-&gt;] (h1_6) -- (h2_0);
    \draw[-&gt;] (h1_0) -- (h2_1);
    \draw[-&gt;] (h1_1) -- (h2_1);
    \draw[-&gt;] (h1_2) -- (h2_1);
    \draw[-&gt;] (h1_3) -- (h2_1);
    \draw[-&gt;] (h1_4) -- (h2_1);
    \draw[-&gt;] (h1_5) -- (h2_1);
    \draw[-&gt;] (h1_6) -- (h2_1);
    \draw[-&gt;] (h1_0) -- (h2_2);
    \draw[-&gt;] (h1_1) -- (h2_2);
    \draw[-&gt;] (h1_2) -- (h2_2);
    \draw[-&gt;] (h1_3) -- (h2_2);
    \draw[-&gt;] (h1_4) -- (h2_2);
    \draw[-&gt;] (h1_5) -- (h2_2);
    \draw[-&gt;] (h1_6) -- (h2_2);
    \draw[-&gt;] (h1_0) -- (h2_3);
    \draw[-&gt;] (h1_1) -- (h2_3);
    \draw[-&gt;] (h1_2) -- (h2_3);
    \draw[-&gt;] (h1_3) -- (h2_3);
    \draw[-&gt;] (h1_4) -- (h2_3);
    \draw[-&gt;] (h1_5) -- (h2_3);
    \draw[-&gt;] (h1_6) -- (h2_3);
    \draw[-&gt;] (h1_0) -- (h2_4);
    \draw[-&gt;] (h1_1) -- (h2_4);
    \draw[-&gt;] (h1_2) -- (h2_4);
    \draw[-&gt;] (h1_3) -- (h2_4);
    \draw[-&gt;] (h1_4) -- (h2_4);
    \draw[-&gt;] (h1_5) -- (h2_4);
    \draw[-&gt;] (h1_6) -- (h2_4);
    \draw[-&gt;] (h1_0) -- (h2_5);
    \draw[-&gt;] (h1_1) -- (h2_5);
    \draw[-&gt;] (h1_2) -- (h2_5);
    \draw[-&gt;] (h1_3) -- (h2_5);
    \draw[-&gt;] (h1_4) -- (h2_5);
    \draw[-&gt;] (h1_5) -- (h2_5);
    \draw[-&gt;] (h1_6) -- (h2_5);
    \draw[-&gt;] (h1_0) -- (h2_6);
    \draw[-&gt;] (h1_1) -- (h2_6);
    \draw[-&gt;] (h1_2) -- (h2_6);
    \draw[-&gt;] (h1_3) -- (h2_6);
    \draw[-&gt;] (h1_4) -- (h2_6);
    \draw[-&gt;] (h1_5) -- (h2_6);
    \draw[-&gt;] (h1_6) -- (h2_6);

    \draw[-&gt;] (h2_0) -- (out_0);
    \draw[-&gt;] (h2_1) -- (out_0);
    \draw[-&gt;] (h2_2) -- (out_0);
    \draw[-&gt;] (h2_3) -- (out_0);
    \draw[-&gt;] (h2_4) -- (out_0);
    \draw[-&gt;] (h2_5) -- (out_0);
    \draw[-&gt;] (h2_6) -- (out_0);


    \node[fill=white] (beta0) at  (1.5, 2) {$\mathbf{w^{(0)}}$};
    \node[fill=white] (beta1) at  (4.5, 2) {$\mathbf{w^{(1)}}$};
    \node[fill=white] (beta2) at  (7.5, 2) {$\mathbf{w^{(2)}}$};</span>)</p>
<p>!pdf2svg command cannot be run</p>
</div>
</div><p>The above graphical representation corresponds to the following model:</p>
<div class="amsmath math notranslate nohighlight" id="equation-10ea99e6-cbef-4130-bcf6-fa52b8fc9914">
<span class="eqno">(1)<a class="headerlink" href="#equation-10ea99e6-cbef-4130-bcf6-fa52b8fc9914" title="Permalink to this equation">#</a></span>\[\begin{align}
  {\color[rgb]{0,1,0}\hat{y}} &amp;= \varphi_\text{out} \left( \sum_i w^{(2)}_{i} {\color{teal}h^{(2)}_{i}} + b^{(2)} \right) \\
  \forall i, {\color{teal}h^{(2)}_{i}} &amp;= \varphi \left( \sum_j w^{(1)}_{ij} {\color[rgb]{0.16,0.61,0.91}h^{(1)}_{j}} + b^{(1)}_{i} \right) \\
  \forall i, {\color[rgb]{0.16,0.61,0.91}h^{(1)}_{i}} &amp;= \varphi \left( \sum_j w^{(0)}_{ij} {\color{blue}x_{j}} + b^{(0)}_{i} \right)
  \label{eq:mlp_2hidden}
\end{align}\]</div>
<p>To be even more precise, the bias terms <span class="math notranslate nohighlight">\(b^{(l)}_i\)</span> are not represented in the graphical representation above.</p>
<p>Such models with one or more hidden layers are called <strong>Multi Layer Perceptrons</strong> (MLP).</p>
</section>
<section id="deciding-on-an-mlp-architecture">
<h2>Deciding on an MLP architecture<a class="headerlink" href="#deciding-on-an-mlp-architecture" title="Permalink to this headline">#</a></h2>
<p>When designing a Multi-Layer Perceptron model to be used for a specific problem, some quantities are fixed by the problem at hand and other are left as hyper-parameters.</p>
<p>Let us take the example of the well-known Iris classification dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/iris.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">iris</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>2</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div></div></div>
</div>
<p>The goal here is to learn how to infer the <code class="docutils literal notranslate"><span class="pre">target</span></code> attribute (3 different possible classes) from the information in the 4 other attributes.</p>
<p>The structure of this dataset dictates:</p>
<ul class="simple">
<li><p>the number of neurons in the input layer, which is equal to the number of descriptive attributes in our dataset (here, 4), and</p></li>
<li><p>the number of neurons in the output layer, which is here equal to 3, since the model is expected to output one probability per target class.</p></li>
</ul>
<p>In more generality, for the output layer, one might face several situations:</p>
<ul class="simple">
<li><p>when regression is at stake, the number of neurons in the output layer is equal to the number of features to be predicted by the model,</p></li>
<li><p>when it comes to classification</p>
<ul>
<li><p>in the case of binary classification, the model will have a single output neuron which will indicate the probability of the positive class</p></li>
<li><p>in the case of multi-class classification, the model will have as many output neurons as the number of classes in the problem.</p></li>
</ul>
</li>
</ul>
<p>Once these number of input / output neurons are fixed, the number of hidden neurons as well as the number of neurons per hidden layer are left as hyper-parameters of the model.</p>
</section>
<section id="activation-functions">
<h2>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h2>
<p>Another important hyper-parameter of neural networks is the choice of the activation function <span class="math notranslate nohighlight">\(\varphi\)</span>.</p>
<p>Here, it is important to notice that if we used the identity function as our activation function, then whatever the depth of our MLP, we would fall back to covering only the family of linear models.
In practice, we will then use activation functions that have some linear regime but don’t behave like a linear function on the whole range of input values.</p>
<p>Historically, the following activation functions have been proposed :</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \text{tanh}(x) =&amp; \frac{2}{1 + e^{-2x}} - 1 \\
    \text{sigmoid}(x) =&amp; \frac{1}{1 + e^{-x}} \\
    \text{ReLU}(x) =&amp; \begin{cases}
                        x \text{ if } x \gt 0\\
                        0 \text{ otherwise }
                      \end{cases}
\end{align*}\]</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;svg&#39;
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ion</span><span class="p">();</span>

<span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">2.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span> <span class="o">-</span> <span class="mf">1.</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">y</span><span class="p">[</span><span class="n">y</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;on&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;on&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;on&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;ReLU&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/mlp_3_0.svg" src="../_images/mlp_3_0.svg" /></div>
</div>
<p>In practice the ReLU function (and some of its variants) is the most widely used nowadays, for reasons that will be discussed in more details in <a class="reference internal" href="optim.html#sec-sgd"><span class="std std-ref">our chapter dedicated to optimization</span></a>.</p>
<section id="the-special-case-of-the-output-layer">
<h3>The special case of the output layer<a class="headerlink" href="#the-special-case-of-the-output-layer" title="Permalink to this headline">#</a></h3>
<p>You might have noticed that in the MLP formulation provided in Equation (1), the output layer has its own activation function, denoted <span class="math notranslate nohighlight">\(\varphi_\text{out}\)</span>.
This is because the choice of activation functions for the output layer of a neural network is a bit specific to the problem at hand.</p>
<p>Indeed, you might have seen that the activation functions discussed in the previous section do not share the same range of output values.
It is hence of prime importance to pick an adequate activation function for the output layer such that our models outputs values that are consistent to the quantities it is supposed to predict.</p>
<p>If, for example, our model was supposed to be used in the Boston Housing dataset we discussed <a class="reference internal" href="perceptron.html#sec-boston"><span class="std std-ref">in the previous chapter</span></a>.
In this case, the goal is to predict housing prices, which are expected to be nonnegative quantities.
It would then be a good idea to use ReLU (which can output any positive value) as the activation function for the output layer in this case.</p>
<p>As stated earlier, in the case of binary classification, the model will have a single output neuron and this neuron will output the probability associated to the positive class.
This quantity is expected to lie in the <span class="math notranslate nohighlight">\([0, 1]\)</span> interval, and the sigmoid activation function is then the default choice in this setting.</p>
<p>Finally, when multi-class classification is at stake, we have one neuron per output class and each neuron is expected to output the probability for a given class.
In this context, the output values should be between 0 and 1, and they should sum to 1.
For this purpose, we use the softmax activation function defined as:</p>
<div class="math notranslate nohighlight">
\[
  \forall i, \text{softmax}(o_i) = \frac{e^{o_i}}{\sum_j e^{o_j}}
\]</div>
<p>where, for all <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(o_i\)</span>’s are the values of the output neurons before applying the activation function.</p>
</section>
</section>
<section id="declaring-an-mlp-in-keras">
<h2>Declaring an MLP in <code class="docutils literal notranslate"><span class="pre">keras</span></code><a class="headerlink" href="#declaring-an-mlp-in-keras" title="Permalink to this headline">#</a></h2>
<p>In order to define a MLP model in <code class="docutils literal notranslate"><span class="pre">keras</span></code>, one just has to stack layers.
As an example, if one wants to code a model made of:</p>
<ul class="simple">
<li><p>an input layer with 10 neurons,</p></li>
<li><p>a hidden layer made of 20 neurons with ReLU activation,</p></li>
<li><p>an output layer made of 3 neurons with softmax activation,</p></li>
</ul>
<p>the code will look like:</p>
<div class="cell tag_remove-stderr docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">InputLayer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">)),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 20)                220       
                                                                 
 dense_1 (Dense)             (None, 3)                 63        
                                                                 
=================================================================
Total params: 283
Trainable params: 283
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> provides an interesting overview of a defined model and its parameters.</p>
<div class="admonition-exercise-1 admonition">
<p class="admonition-title">Exercise #1</p>
<p>Relying on what we have seen in this chapter, can you explain the number of parameters returned by <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> above?</p>
<div class="dropdown tip admonition">
<p class="admonition-title">Solution</p>
<p>Our input layer is made of 10 neurons, and our first layer is fully connected, hence each of these neurons is connected to a neuron in the hidden layer through a parameter, which already makes <span class="math notranslate nohighlight">\(10 \times 20 = 200\)</span> parameters.
Moreover, each of the hidden layer neurons has its own bias parameter, which is <span class="math notranslate nohighlight">\(20\)</span> more parameters.
We then have 220 parameters, as output by <code class="docutils literal notranslate"><span class="pre">model.summary()</span></code> for the layer <code class="docutils literal notranslate"><span class="pre">&quot;dense</span> <span class="pre">(Dense)&quot;</span></code>.</p>
<p>Similarly, for the connection of the hidden layer neurons to those in the output layer, the total number of parameters is <span class="math notranslate nohighlight">\(20 \times 3 = 60\)</span> for the weights plus <span class="math notranslate nohighlight">\(3\)</span> extra parameters for the biases.</p>
<p>Overall, we have <span class="math notranslate nohighlight">\(220 + 63 = 283\)</span> parameters in this model.</p>
</div>
</div>
<div class="admonition-exercise-2 admonition">
<p class="admonition-title">Exercise #2</p>
<p>Declare, in <code class="docutils literal notranslate"><span class="pre">keras</span></code>, an MLP with one hidden layer made of 100 neurons and ReLU activation for the Iris dataset presented above.</p>
<div class="dropdown tip admonition">
<p class="admonition-title">Solution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">)),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition-exercise-3 admonition">
<p class="admonition-title">Exercise #3</p>
<p>Same question for the full Boston Housing dataset shown below (the goal here is to predict the <code class="docutils literal notranslate"><span class="pre">PRICE</span></code> feature based on the other ones).</p>
<div class="dropdown tip admonition">
<p class="admonition-title">Solution</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
    <span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="p">)),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">boston</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/boston.csv&quot;</span><span class="p">)[[</span><span class="s2">&quot;RM&quot;</span><span class="p">,</span> <span class="s2">&quot;CRIM&quot;</span><span class="p">,</span> <span class="s2">&quot;INDUS&quot;</span><span class="p">,</span> <span class="s2">&quot;NOX&quot;</span><span class="p">,</span> <span class="s2">&quot;AGE&quot;</span><span class="p">,</span> <span class="s2">&quot;TAX&quot;</span><span class="p">,</span> <span class="s2">&quot;PRICE&quot;</span><span class="p">]]</span>
<span class="n">boston</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RM</th>
      <th>CRIM</th>
      <th>INDUS</th>
      <th>NOX</th>
      <th>AGE</th>
      <th>TAX</th>
      <th>PRICE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6.575</td>
      <td>0.00632</td>
      <td>2.31</td>
      <td>0.538</td>
      <td>65.2</td>
      <td>296.0</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.421</td>
      <td>0.02731</td>
      <td>7.07</td>
      <td>0.469</td>
      <td>78.9</td>
      <td>242.0</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.185</td>
      <td>0.02729</td>
      <td>7.07</td>
      <td>0.469</td>
      <td>61.1</td>
      <td>242.0</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6.998</td>
      <td>0.03237</td>
      <td>2.18</td>
      <td>0.458</td>
      <td>45.8</td>
      <td>222.0</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.147</td>
      <td>0.06905</td>
      <td>2.18</td>
      <td>0.458</td>
      <td>54.2</td>
      <td>222.0</td>
      <td>36.2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>501</th>
      <td>6.593</td>
      <td>0.06263</td>
      <td>11.93</td>
      <td>0.573</td>
      <td>69.1</td>
      <td>273.0</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>502</th>
      <td>6.120</td>
      <td>0.04527</td>
      <td>11.93</td>
      <td>0.573</td>
      <td>76.7</td>
      <td>273.0</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>503</th>
      <td>6.976</td>
      <td>0.06076</td>
      <td>11.93</td>
      <td>0.573</td>
      <td>91.0</td>
      <td>273.0</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>504</th>
      <td>6.794</td>
      <td>0.10959</td>
      <td>11.93</td>
      <td>0.573</td>
      <td>89.3</td>
      <td>273.0</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>505</th>
      <td>6.030</td>
      <td>0.04741</td>
      <td>11.93</td>
      <td>0.573</td>
      <td>80.8</td>
      <td>273.0</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
<p>506 rows × 7 columns</p>
</div></div></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="perceptron.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="loss.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Losses</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Romain Tavenard<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>